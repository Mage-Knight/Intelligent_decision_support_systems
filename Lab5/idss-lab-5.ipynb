{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторна робота 5 з ІСППР\n",
    "### Виконали студенти групи КІ-31мп Шабо О.А. та Сотник Д.C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Частина 1\n",
    "\n",
    "### 1. Обрати два невеликі тексти з 4 - 5 речень - один на англійській мові, інший - на українській."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_eng = \"\"\n",
    "orig_ukr = \"\"\n",
    "\n",
    "with open(\"0_eng.txt\", \"r\") as file:\n",
    "    orig_eng = file.read()\n",
    "    \n",
    "with open(\"0_ukr.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    orig_ukr = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Луцькі луки - це особливий куточок природи, який зачаровує своєю красою та спокоєм',\n",
       " 'Розташовані вони на околицях міста Луцька, і завдяки своїй величезній площі створюють враження нескінченної просторовості',\n",
       " \"У цій природній зоні можна зустріти різноманітні види рослин та тварин, які розфарбовують луки в різнобарв'я кожної пори року\",\n",
       " 'Прогулянки по луцьких луках стають справжнім відпочинком для душі та джерелом натхнення для творчості.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_text(text):\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    sentences = text.split(\". \")\n",
    "    return sentences\n",
    "\n",
    "eng = process_text(orig_eng)\n",
    "ukr = process_text(orig_ukr)\n",
    "\n",
    "ukr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Виконати векторизацію речень - побудувати вектори числових ознак для речень, використовуючи:\n",
    "- модель bag-of-words,\n",
    "- моделі n-грам\n",
    "- їх реалізацію в scikit-learn: клас CountVectorizer модуля sklearn.feature_extraction.text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def preprocess(sentence):\n",
    "    # Приводимо речення до нижнього регістру та видаляємо пунктуацію\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    sentence = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return sentence\n",
    "\n",
    "def create_bag_of_words(sentences):\n",
    "    # Підрахунок слів у кожному реченні\n",
    "    word_counts = []\n",
    "    for sentence in sentences:\n",
    "        sentence = preprocess(sentence)\n",
    "        words = sentence.split()\n",
    "        word_count = Counter(words)\n",
    "        word_counts.append(word_count)\n",
    "    \n",
    "    # Створення словника унікальних слів\n",
    "    vocab = set()\n",
    "    for word_count in word_counts:\n",
    "        vocab.update(word_count.keys())\n",
    "    vocab = sorted(vocab)\n",
    "    \n",
    "    # Створення bag-of-words векторів\n",
    "    bag_of_words_vectors = []\n",
    "    for word_count in word_counts:\n",
    "        vector = [word_count.get(word, 0) for word in vocab]\n",
    "        bag_of_words_vectors.append(vector)\n",
    "    \n",
    "    return vocab, bag_of_words_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'affair', 'and', 'aroma', 'around', 'basil', 'bringing', 'comforting', 'connection', 'creating', 'dinner', 'fades', 'fork', 'fresh', 'has', 'heart', 'it', 'its', 'love', 'meatballs', 'moments', 'never', 'of', 'on', 'or', 'parmesan', 'people', 'sauce', 'simmering', 'simplicity', 'something', 'spaghetti', 'speaks', 'sprinkle', 'stove', 'table', 'that', 'the', 'theres', 'to', 'together', 'tomato', 'topped', 'twirls', 'warmth', 'way', 'whether', 'with']\n",
      "There's something about the simplicity of spaghetti that speaks to the heart\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "The way it twirls on the fork, the comforting aroma of tomato sauce simmering on the stove\n",
      "[0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 4, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0]\n",
      "It's a love affair that never fades\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Whether topped with meatballs, fresh basil, or a sprinkle of Parmesan, spaghetti has a way of bringing people together, creating moments of warmth and connection around the dinner table.\n",
      "[0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 3, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "word_index, bag_of_words = create_bag_of_words(eng)\n",
    "print(word_index)\n",
    "for sentence, bow in zip(eng, bag_of_words):\n",
    "    print(sentence)\n",
    "    print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['величезній', 'види', 'вони', 'враження', 'відпочинком', 'джерелом', 'для', 'душі', 'завдяки', 'зачаровує', 'зоні', 'зустріти', 'кожної', 'красою', 'куточок', 'луках', 'луки', 'луцька', 'луцьких', 'луцькі', 'можна', 'міста', 'на', 'натхнення', 'нескінченної', 'околицях', 'особливий', 'площі', 'по', 'пори', 'природи', 'природній', 'прогулянки', 'просторовості', 'розташовані', 'розфарбовують', 'року', 'рослин', 'різнобарвя', 'різноманітні', 'своєю', 'своїй', 'спокоєм', 'справжнім', 'стають', 'створюють', 'та', 'тварин', 'творчості', 'це', 'цій', 'який', 'які']\n",
      "Луцькі луки - це особливий куточок природи, який зачаровує своєю красою та спокоєм\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0]\n",
      "Розташовані вони на околицях міста Луцька, і завдяки своїй величезній площі створюють враження нескінченної просторовості\n",
      "[1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "У цій природній зоні можна зустріти різноманітні види рослин та тварин, які розфарбовують луки в різнобарв'я кожної пори року\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1]\n",
      "Прогулянки по луцьких луках стають справжнім відпочинком для душі та джерелом натхнення для творчості.\n",
      "[0, 0, 0, 0, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "word_index, bag_of_words = create_bag_of_words(ukr)\n",
    "print(word_index)\n",
    "for sentence, bow in zip(ukr, bag_of_words):\n",
    "    print(sentence)\n",
    "    print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Словник: ['about' 'affair' 'and' 'aroma' 'around' 'basil' 'bringing' 'comforting'\n",
      " 'connection' 'creating' 'dinner' 'fades' 'fork' 'fresh' 'has' 'heart'\n",
      " 'it' 'love' 'meatballs' 'moments' 'never' 'of' 'on' 'or' 'parmesan'\n",
      " 'people' 'sauce' 'simmering' 'simplicity' 'something' 'spaghetti'\n",
      " 'speaks' 'sprinkle' 'stove' 'table' 'that' 'the' 'there' 'to' 'together'\n",
      " 'tomato' 'topped' 'twirls' 'warmth' 'way' 'whether' 'with']\n",
      "Bag-of-Words векторизація:\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1\n",
      "  2 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 2 0 0 0 1 1 0 0 0 0 0 1 0 0\n",
      "  4 0 0 0 1 0 1 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 1 0 3 0 1 1 1 0 0 0 0 1 0 1 0 1 0\n",
      "  1 0 0 1 0 1 0 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bow_eng = vectorizer.fit_transform(eng)\n",
    "\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "bag_of_words = bow_eng.toarray()\n",
    "\n",
    "print(\"Словник:\", vocab)\n",
    "print(\"Bag-of-Words векторизація:\")\n",
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Словник: ['величезній' 'види' 'вони' 'враження' 'відпочинком' 'джерелом' 'для'\n",
      " 'душі' 'завдяки' 'зачаровує' 'зоні' 'зустріти' 'кожної' 'красою'\n",
      " 'куточок' 'луках' 'луки' 'луцька' 'луцьких' 'луцькі' 'можна' 'міста' 'на'\n",
      " 'натхнення' 'нескінченної' 'околицях' 'особливий' 'площі' 'по' 'пори'\n",
      " 'природи' 'природній' 'прогулянки' 'просторовості' 'розташовані'\n",
      " 'розфарбовують' 'року' 'рослин' 'різнобарв' 'різноманітні' 'своєю'\n",
      " 'своїй' 'спокоєм' 'справжнім' 'стають' 'створюють' 'та' 'тварин'\n",
      " 'творчості' 'це' 'цій' 'який' 'які']\n",
      "Bag-of-Words векторизація:\n",
      "[[0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0]\n",
      " [1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0\n",
      "  0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1\n",
      "  1 1 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1]\n",
      " [0 0 0 0 1 1 2 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bow_ukr = vectorizer.fit_transform(ukr)\n",
    "\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "bag_of_words = bow_ukr.toarray()\n",
    "\n",
    "print(\"Словник:\", vocab)\n",
    "print(\"Bag-of-Words векторизація:\")\n",
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-грам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Словник біграм:\n",
      "['about the', 'affair that', 'and connection', 'aroma of', 'around the', 'basil or', 'bringing people', 'comforting aroma', 'connection around', 'creating moments', 'dinner table', 'fork the', 'fresh basil', 'has way', 'it love', 'it twirls', 'love affair', 'meatballs fresh', 'moments of', 'never fades', 'of bringing', 'of parmesan', 'of spaghetti', 'of tomato', 'of warmth', 'on the', 'or sprinkle', 'parmesan spaghetti', 'people together', 'sauce simmering', 'simmering on', 'simplicity of', 'something about', 'spaghetti has', 'spaghetti that', 'speaks to', 'sprinkle of', 'that never', 'that speaks', 'the comforting', 'the dinner', 'the fork', 'the heart', 'the simplicity', 'the stove', 'the way', 'there something', 'to the', 'together creating', 'tomato sauce', 'topped with', 'twirls on', 'warmth and', 'way it', 'way of', 'whether topped', 'with meatballs']\n",
      "N-грам векторизація:\n",
      "Речення 1: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Речення 2: [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0]\n",
      "Речення 3: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Речення 4: [0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    # Токенізація тексту\n",
    "    words = re.findall(r'\\b\\w{2,}\\b', text.lower())\n",
    "    return words\n",
    "\n",
    "def generate_ngrams(tokens, n):\n",
    "    # Генерація n-грам з токенів\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = \" \".join(tokens[i:i+n])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "def ngram_vectorize(sentences, n):\n",
    "    # Побудова словника унікальних n-грам\n",
    "    vocab = set()\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenize(sentence)\n",
    "        ngrams = generate_ngrams(tokens, n)\n",
    "        vocab.update(ngrams)\n",
    "    vocab = sorted(vocab)\n",
    "    \n",
    "    # Створення n-грам векторів\n",
    "    ngram_vectors = []\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenize(sentence)\n",
    "        ngrams = generate_ngrams(tokens, n)\n",
    "        vector = [ngrams.count(ngram) for ngram in vocab]\n",
    "        ngram_vectors.append(vector)\n",
    "    \n",
    "    return vocab, ngram_vectors\n",
    "\n",
    "n = 2\n",
    "vocab, ngram_vectors = ngram_vectorize(eng, n)\n",
    "\n",
    "# Виведення результатів\n",
    "print(\"Словник біграм:\")\n",
    "print(vocab)\n",
    "print(\"N-грам векторизація:\")\n",
    "for i, vector in enumerate(ngram_vectors, 1):\n",
    "    print(f\"Речення {i}: {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Словник біграм:\n",
      "['величезній площі', 'види рослин', 'вони на', 'враження нескінченної', 'відпочинком для', 'джерелом натхнення', 'для душі', 'для творчості', 'душі та', 'завдяки своїй', 'зачаровує своєю', 'зоні можна', 'зустріти різноманітні', 'кожної пори', 'красою та', 'куточок природи', 'луках стають', 'луки різнобарв', 'луки це', 'луцька завдяки', 'луцьких луках', 'луцькі луки', 'можна зустріти', 'міста луцька', 'на околицях', 'натхнення для', 'нескінченної просторовості', 'околицях міста', 'особливий куточок', 'площі створюють', 'по луцьких', 'пори року', 'природи який', 'природній зоні', 'прогулянки по', 'розташовані вони', 'розфарбовують луки', 'рослин та', 'різнобарв кожної', 'різноманітні види', 'своєю красою', 'своїй величезній', 'справжнім відпочинком', 'стають справжнім', 'створюють враження', 'та джерелом', 'та спокоєм', 'та тварин', 'тварин які', 'це особливий', 'цій природній', 'який зачаровує', 'які розфарбовують']\n",
      "N-грам векторизація:\n",
      "Речення 1: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0]\n",
      "Речення 2: [1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Речення 3: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1]\n",
      "Речення 4: [0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "vocab, ngram_vectors = ngram_vectorize(ukr, n)\n",
    "\n",
    "# Виведення результатів\n",
    "print(\"Словник біграм:\")\n",
    "print(vocab)\n",
    "print(\"N-грам векторизація:\")\n",
    "for i, vector in enumerate(ngram_vectors, 1):\n",
    "    print(f\"Речення {i}: {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Словник біграм:\n",
      "['about the' 'affair that' 'and connection' 'aroma of' 'around the'\n",
      " 'basil or' 'bringing people' 'comforting aroma' 'connection around'\n",
      " 'creating moments' 'dinner table' 'fork the' 'fresh basil' 'has way'\n",
      " 'it love' 'it twirls' 'love affair' 'meatballs fresh' 'moments of'\n",
      " 'never fades' 'of bringing' 'of parmesan' 'of spaghetti' 'of tomato'\n",
      " 'of warmth' 'on the' 'or sprinkle' 'parmesan spaghetti' 'people together'\n",
      " 'sauce simmering' 'simmering on' 'simplicity of' 'something about'\n",
      " 'spaghetti has' 'spaghetti that' 'speaks to' 'sprinkle of' 'that never'\n",
      " 'that speaks' 'the comforting' 'the dinner' 'the fork' 'the heart'\n",
      " 'the simplicity' 'the stove' 'the way' 'there something' 'to the'\n",
      " 'together creating' 'tomato sauce' 'topped with' 'twirls on' 'warmth and'\n",
      " 'way it' 'way of' 'whether topped' 'with meatballs']\n",
      "N-грам векторизація:\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1\n",
      "  0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 2 0 0 0 1 1 0 0 0 0 0\n",
      "  0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 0 1 1 0 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0\n",
      "  1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# Викликаємо fit_transform, щоб навчити та застосувати векторизацію\n",
    "vec_bigram_eng = vectorizer.fit_transform(eng)\n",
    "\n",
    "# Отримуємо словник та згенеровану матрицю\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "ngram_vectors = vec_bigram_eng.toarray()\n",
    "\n",
    "# Виводимо результати\n",
    "print(\"Словник біграм:\")\n",
    "print(vocab)\n",
    "print(\"N-грам векторизація:\")\n",
    "print(ngram_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Словник біграм:\n",
      "['величезній площі' 'види рослин' 'вони на' 'враження нескінченної'\n",
      " 'відпочинком для' 'джерелом натхнення' 'для душі' 'для творчості'\n",
      " 'душі та' 'завдяки своїй' 'зачаровує своєю' 'зоні можна'\n",
      " 'зустріти різноманітні' 'кожної пори' 'красою та' 'куточок природи'\n",
      " 'луках стають' 'луки різнобарв' 'луки це' 'луцька завдяки'\n",
      " 'луцьких луках' 'луцькі луки' 'можна зустріти' 'міста луцька'\n",
      " 'на околицях' 'натхнення для' 'нескінченної просторовості'\n",
      " 'околицях міста' 'особливий куточок' 'площі створюють' 'по луцьких'\n",
      " 'пори року' 'природи який' 'природній зоні' 'прогулянки по'\n",
      " 'розташовані вони' 'розфарбовують луки' 'рослин та' 'різнобарв кожної'\n",
      " 'різноманітні види' 'своєю красою' 'своїй величезній'\n",
      " 'справжнім відпочинком' 'стають справжнім' 'створюють враження'\n",
      " 'та джерелом' 'та спокоєм' 'та тварин' 'тварин які' 'це особливий'\n",
      " 'цій природній' 'який зачаровує' 'які розфарбовують']\n",
      "N-грам векторизація:\n",
      "[[0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0]\n",
      " [1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 1\n",
      "  0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      "  1 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0 1]\n",
      " [0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0\n",
      "  0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "vec_bigram_ukr = vectorizer.fit_transform(ukr)\n",
    "\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "ngram_vectors = vec_bigram_ukr.toarray()\n",
    "\n",
    "# Виводимо результати\n",
    "print(\"Словник біграм:\")\n",
    "print(vocab)\n",
    "print(\"N-грам векторизація:\")\n",
    "print(ngram_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оцінити важливість слів за допомогою методу tf-idf (term frequency-inverse document frequency), використовуючи клас TfidfTransformer модуля sklearn.feature_extraction.text.\n",
    "Розглянути різні значення параметрів методу tf-idf:\n",
    "- norm,\n",
    "- use_idf,\n",
    "- smooth_idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF з параметрами norm='l2', use_idf=True, smooth_idf=True:\n",
      "[[0.31188783 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.31188783 0.         0.\n",
      "  0.         0.         0.         0.19907396 0.         0.\n",
      "  0.         0.         0.         0.         0.31188783 0.31188783\n",
      "  0.24589595 0.31188783 0.         0.         0.         0.24589595\n",
      "  0.39814793 0.31188783 0.31188783 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.22266711 0.         0.\n",
      "  0.         0.22266711 0.         0.         0.         0.\n",
      "  0.22266711 0.         0.         0.         0.17555331 0.\n",
      "  0.         0.         0.         0.14212553 0.44533423 0.\n",
      "  0.         0.         0.22266711 0.22266711 0.         0.\n",
      "  0.         0.         0.         0.22266711 0.         0.\n",
      "  0.56850214 0.         0.         0.         0.22266711 0.\n",
      "  0.22266711 0.         0.17555331 0.         0.        ]\n",
      " [0.         0.43671931 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.43671931\n",
      "  0.         0.         0.         0.         0.34431452 0.43671931\n",
      "  0.         0.         0.43671931 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.34431452\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.19493033 0.         0.19493033 0.19493033\n",
      "  0.19493033 0.         0.19493033 0.19493033 0.19493033 0.\n",
      "  0.         0.19493033 0.19493033 0.         0.         0.\n",
      "  0.19493033 0.19493033 0.         0.37326452 0.         0.19493033\n",
      "  0.19493033 0.19493033 0.         0.         0.         0.\n",
      "  0.15368531 0.         0.19493033 0.         0.19493033 0.\n",
      "  0.12442151 0.         0.         0.19493033 0.         0.19493033\n",
      "  0.         0.19493033 0.15368531 0.19493033 0.19493033]]\n",
      "\n",
      "TF-IDF з параметрами norm=None, use_idf=True, smooth_idf=False:\n",
      "[[2.38629436 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         2.38629436 0.         0.\n",
      "  0.         0.         0.         1.28768207 0.         0.\n",
      "  0.         0.         0.         0.         2.38629436 2.38629436\n",
      "  1.69314718 2.38629436 0.         0.         0.         1.69314718\n",
      "  2.57536414 2.38629436 2.38629436 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         2.38629436 0.         0.\n",
      "  0.         2.38629436 0.         0.         0.         0.\n",
      "  2.38629436 0.         0.         0.         1.69314718 0.\n",
      "  0.         0.         0.         1.28768207 4.77258872 0.\n",
      "  0.         0.         2.38629436 2.38629436 0.         0.\n",
      "  0.         0.         0.         2.38629436 0.         0.\n",
      "  5.15072829 0.         0.         0.         2.38629436 0.\n",
      "  2.38629436 0.         1.69314718 0.         0.        ]\n",
      " [0.         2.38629436 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         2.38629436\n",
      "  0.         0.         0.         0.         1.69314718 2.38629436\n",
      "  0.         0.         2.38629436 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.69314718\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         2.38629436 0.         2.38629436 2.38629436\n",
      "  2.38629436 0.         2.38629436 2.38629436 2.38629436 0.\n",
      "  0.         2.38629436 2.38629436 0.         0.         0.\n",
      "  2.38629436 2.38629436 0.         3.86304622 0.         2.38629436\n",
      "  2.38629436 2.38629436 0.         0.         0.         0.\n",
      "  1.69314718 0.         2.38629436 0.         2.38629436 0.\n",
      "  1.28768207 0.         0.         2.38629436 0.         2.38629436\n",
      "  0.         2.38629436 1.69314718 2.38629436 2.38629436]]\n",
      "\n",
      "TF-IDF з параметрами norm='l2', use_idf=False, smooth_idf=True:\n",
      "[[0.26726124 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.26726124 0.         0.\n",
      "  0.         0.         0.         0.26726124 0.         0.\n",
      "  0.         0.         0.         0.         0.26726124 0.26726124\n",
      "  0.26726124 0.26726124 0.         0.         0.         0.26726124\n",
      "  0.53452248 0.26726124 0.26726124 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.1796053  0.         0.\n",
      "  0.         0.1796053  0.         0.         0.         0.\n",
      "  0.1796053  0.         0.         0.         0.1796053  0.\n",
      "  0.         0.         0.         0.1796053  0.3592106  0.\n",
      "  0.         0.         0.1796053  0.1796053  0.         0.\n",
      "  0.         0.         0.         0.1796053  0.         0.\n",
      "  0.71842121 0.         0.         0.         0.1796053  0.\n",
      "  0.1796053  0.         0.1796053  0.         0.        ]\n",
      " [0.         0.40824829 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.40824829\n",
      "  0.         0.         0.         0.         0.40824829 0.40824829\n",
      "  0.         0.         0.40824829 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.40824829\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.17407766 0.         0.17407766 0.17407766\n",
      "  0.17407766 0.         0.17407766 0.17407766 0.17407766 0.\n",
      "  0.         0.17407766 0.17407766 0.         0.         0.\n",
      "  0.17407766 0.17407766 0.         0.52223297 0.         0.17407766\n",
      "  0.17407766 0.17407766 0.         0.         0.         0.\n",
      "  0.17407766 0.         0.17407766 0.         0.17407766 0.\n",
      "  0.17407766 0.         0.         0.17407766 0.         0.17407766\n",
      "  0.         0.17407766 0.17407766 0.17407766 0.17407766]]\n",
      "TF-IDF з параметрами norm='l2', use_idf=True, smooth_idf=True:\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.30111464 0.         0.\n",
      "  0.         0.30111464 0.30111464 0.         0.23740224 0.\n",
      "  0.         0.30111464 0.         0.         0.         0.\n",
      "  0.         0.         0.30111464 0.         0.         0.\n",
      "  0.30111464 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.30111464 0.\n",
      "  0.30111464 0.         0.         0.         0.19219757 0.\n",
      "  0.         0.30111464 0.         0.30111464 0.        ]\n",
      " [0.26726124 0.         0.26726124 0.26726124 0.         0.\n",
      "  0.         0.         0.26726124 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.26726124\n",
      "  0.         0.         0.         0.26726124 0.26726124 0.\n",
      "  0.26726124 0.26726124 0.         0.26726124 0.         0.\n",
      "  0.         0.         0.         0.26726124 0.26726124 0.\n",
      "  0.         0.         0.         0.         0.         0.26726124\n",
      "  0.         0.         0.         0.26726124 0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.24977372 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.24977372 0.24977372\n",
      "  0.24977372 0.         0.         0.         0.19692447 0.\n",
      "  0.         0.         0.24977372 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.24977372\n",
      "  0.         0.24977372 0.         0.         0.         0.24977372\n",
      "  0.24977372 0.24977372 0.24977372 0.24977372 0.         0.\n",
      "  0.         0.         0.         0.         0.15942733 0.24977372\n",
      "  0.         0.         0.24977372 0.         0.24977372]\n",
      " [0.         0.         0.         0.         0.25476231 0.25476231\n",
      "  0.50952462 0.25476231 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.25476231 0.         0.\n",
      "  0.25476231 0.         0.         0.         0.         0.25476231\n",
      "  0.         0.         0.         0.         0.25476231 0.\n",
      "  0.         0.         0.25476231 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.25476231 0.25476231 0.         0.16261148 0.\n",
      "  0.25476231 0.         0.         0.         0.        ]]\n",
      "\n",
      "TF-IDF з параметрами norm=None, use_idf=True, smooth_idf=False:\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         2.38629436 0.         0.\n",
      "  0.         2.38629436 2.38629436 0.         1.69314718 0.\n",
      "  0.         2.38629436 0.         0.         0.         0.\n",
      "  0.         0.         2.38629436 0.         0.         0.\n",
      "  2.38629436 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         2.38629436 0.\n",
      "  2.38629436 0.         0.         0.         1.28768207 0.\n",
      "  0.         2.38629436 0.         2.38629436 0.        ]\n",
      " [2.38629436 0.         2.38629436 2.38629436 0.         0.\n",
      "  0.         0.         2.38629436 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         2.38629436\n",
      "  0.         0.         0.         2.38629436 2.38629436 0.\n",
      "  2.38629436 2.38629436 0.         2.38629436 0.         0.\n",
      "  0.         0.         0.         2.38629436 2.38629436 0.\n",
      "  0.         0.         0.         0.         0.         2.38629436\n",
      "  0.         0.         0.         2.38629436 0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         2.38629436 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         2.38629436 2.38629436\n",
      "  2.38629436 0.         0.         0.         1.69314718 0.\n",
      "  0.         0.         2.38629436 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         2.38629436\n",
      "  0.         2.38629436 0.         0.         0.         2.38629436\n",
      "  2.38629436 2.38629436 2.38629436 2.38629436 0.         0.\n",
      "  0.         0.         0.         0.         1.28768207 2.38629436\n",
      "  0.         0.         2.38629436 0.         2.38629436]\n",
      " [0.         0.         0.         0.         2.38629436 2.38629436\n",
      "  4.77258872 2.38629436 0.         0.         0.         0.\n",
      "  0.         0.         0.         2.38629436 0.         0.\n",
      "  2.38629436 0.         0.         0.         0.         2.38629436\n",
      "  0.         0.         0.         0.         2.38629436 0.\n",
      "  0.         0.         2.38629436 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         2.38629436 2.38629436 0.         1.28768207 0.\n",
      "  2.38629436 0.         0.         0.         0.        ]]\n",
      "\n",
      "TF-IDF з параметрами norm='l2', use_idf=False, smooth_idf=True:\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.28867513 0.         0.\n",
      "  0.         0.28867513 0.28867513 0.         0.28867513 0.\n",
      "  0.         0.28867513 0.         0.         0.         0.\n",
      "  0.         0.         0.28867513 0.         0.         0.\n",
      "  0.28867513 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.28867513 0.\n",
      "  0.28867513 0.         0.         0.         0.28867513 0.\n",
      "  0.         0.28867513 0.         0.28867513 0.        ]\n",
      " [0.26726124 0.         0.26726124 0.26726124 0.         0.\n",
      "  0.         0.         0.26726124 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.26726124\n",
      "  0.         0.         0.         0.26726124 0.26726124 0.\n",
      "  0.26726124 0.26726124 0.         0.26726124 0.         0.\n",
      "  0.         0.         0.         0.26726124 0.26726124 0.\n",
      "  0.         0.         0.         0.         0.         0.26726124\n",
      "  0.         0.         0.         0.26726124 0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.24253563 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.24253563 0.24253563\n",
      "  0.24253563 0.         0.         0.         0.24253563 0.\n",
      "  0.         0.         0.24253563 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.24253563\n",
      "  0.         0.24253563 0.         0.         0.         0.24253563\n",
      "  0.24253563 0.24253563 0.24253563 0.24253563 0.         0.\n",
      "  0.         0.         0.         0.         0.24253563 0.24253563\n",
      "  0.         0.         0.24253563 0.         0.24253563]\n",
      " [0.         0.         0.         0.         0.25       0.25\n",
      "  0.5        0.25       0.         0.         0.         0.\n",
      "  0.         0.         0.         0.25       0.         0.\n",
      "  0.25       0.         0.         0.         0.         0.25\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.         0.25       0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.25       0.25       0.         0.25       0.\n",
      "  0.25       0.         0.         0.         0.        ]]\n",
      "TF-IDF з параметрами norm='l2', use_idf=True, smooth_idf=True:\n",
      "[[0.30151134 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.30151134 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.30151134 0.30151134 0.         0.30151134 0.30151134\n",
      "  0.         0.         0.30151134 0.         0.         0.\n",
      "  0.30151134 0.30151134 0.         0.         0.30151134 0.30151134\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.23570226 0.         0.\n",
      "  0.         0.23570226 0.         0.         0.         0.23570226\n",
      "  0.         0.         0.         0.23570226 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.23570226\n",
      "  0.         0.47140452 0.         0.         0.         0.23570226\n",
      "  0.23570226 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.23570226 0.         0.23570226\n",
      "  0.         0.         0.23570226 0.23570226 0.         0.\n",
      "  0.         0.23570226 0.         0.23570226 0.         0.23570226\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.4472136  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.4472136  0.         0.4472136  0.\n",
      "  0.         0.4472136  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.4472136  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.19611614 0.         0.19611614 0.19611614\n",
      "  0.19611614 0.         0.19611614 0.19611614 0.19611614 0.\n",
      "  0.19611614 0.19611614 0.         0.         0.         0.19611614\n",
      "  0.19611614 0.         0.19611614 0.19611614 0.         0.\n",
      "  0.19611614 0.         0.19611614 0.19611614 0.19611614 0.\n",
      "  0.         0.         0.         0.19611614 0.         0.\n",
      "  0.19611614 0.         0.         0.         0.19611614 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.19611614 0.         0.19611614 0.         0.19611614 0.\n",
      "  0.19611614 0.19611614 0.19611614]]\n",
      "\n",
      "TF-IDF з параметрами norm=None, use_idf=True, smooth_idf=False:\n",
      "[[2.38629436 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         2.38629436 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         2.38629436 2.38629436 0.         2.38629436 2.38629436\n",
      "  0.         0.         2.38629436 0.         0.         0.\n",
      "  2.38629436 2.38629436 0.         0.         2.38629436 2.38629436\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         2.38629436 0.         0.\n",
      "  0.         2.38629436 0.         0.         0.         2.38629436\n",
      "  0.         0.         0.         2.38629436 0.         0.\n",
      "  0.         0.         0.         0.         0.         2.38629436\n",
      "  0.         4.77258872 0.         0.         0.         2.38629436\n",
      "  2.38629436 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         2.38629436 0.         2.38629436\n",
      "  0.         0.         2.38629436 2.38629436 0.         0.\n",
      "  0.         2.38629436 0.         2.38629436 0.         2.38629436\n",
      "  0.         0.         0.        ]\n",
      " [0.         2.38629436 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         2.38629436 0.         2.38629436 0.\n",
      "  0.         2.38629436 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         2.38629436 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         2.38629436 0.         2.38629436 2.38629436\n",
      "  2.38629436 0.         2.38629436 2.38629436 2.38629436 0.\n",
      "  2.38629436 2.38629436 0.         0.         0.         2.38629436\n",
      "  2.38629436 0.         2.38629436 2.38629436 0.         0.\n",
      "  2.38629436 0.         2.38629436 2.38629436 2.38629436 0.\n",
      "  0.         0.         0.         2.38629436 0.         0.\n",
      "  2.38629436 0.         0.         0.         2.38629436 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  2.38629436 0.         2.38629436 0.         2.38629436 0.\n",
      "  2.38629436 2.38629436 2.38629436]]\n",
      "\n",
      "TF-IDF з параметрами norm='l2', use_idf=False, smooth_idf=True:\n",
      "[[0.30151134 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.30151134 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.30151134 0.30151134 0.         0.30151134 0.30151134\n",
      "  0.         0.         0.30151134 0.         0.         0.\n",
      "  0.30151134 0.30151134 0.         0.         0.30151134 0.30151134\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.23570226 0.         0.\n",
      "  0.         0.23570226 0.         0.         0.         0.23570226\n",
      "  0.         0.         0.         0.23570226 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.23570226\n",
      "  0.         0.47140452 0.         0.         0.         0.23570226\n",
      "  0.23570226 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.23570226 0.         0.23570226\n",
      "  0.         0.         0.23570226 0.23570226 0.         0.\n",
      "  0.         0.23570226 0.         0.23570226 0.         0.23570226\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.4472136  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.4472136  0.         0.4472136  0.\n",
      "  0.         0.4472136  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.4472136  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.19611614 0.         0.19611614 0.19611614\n",
      "  0.19611614 0.         0.19611614 0.19611614 0.19611614 0.\n",
      "  0.19611614 0.19611614 0.         0.         0.         0.19611614\n",
      "  0.19611614 0.         0.19611614 0.19611614 0.         0.\n",
      "  0.19611614 0.         0.19611614 0.19611614 0.19611614 0.\n",
      "  0.         0.         0.         0.19611614 0.         0.\n",
      "  0.19611614 0.         0.         0.         0.19611614 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.19611614 0.         0.19611614 0.         0.19611614 0.\n",
      "  0.19611614 0.19611614 0.19611614]]\n",
      "TF-IDF з параметрами norm='l2', use_idf=True, smooth_idf=True:\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.30151134 0.\n",
      "  0.         0.         0.30151134 0.30151134 0.         0.\n",
      "  0.30151134 0.         0.         0.30151134 0.         0.\n",
      "  0.         0.         0.         0.         0.30151134 0.\n",
      "  0.         0.         0.30151134 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.30151134 0.\n",
      "  0.         0.         0.         0.         0.30151134 0.\n",
      "  0.         0.30151134 0.         0.30151134 0.        ]\n",
      " [0.2773501  0.         0.2773501  0.2773501  0.         0.\n",
      "  0.         0.         0.         0.2773501  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.2773501  0.         0.         0.         0.2773501\n",
      "  0.2773501  0.         0.2773501  0.2773501  0.         0.2773501\n",
      "  0.         0.         0.         0.         0.         0.2773501\n",
      "  0.         0.         0.         0.         0.         0.2773501\n",
      "  0.         0.         0.2773501  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.25       0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.25\n",
      "  0.25       0.25       0.         0.         0.         0.25\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.25       0.         0.25       0.         0.\n",
      "  0.25       0.25       0.25       0.25       0.         0.\n",
      "  0.         0.         0.         0.         0.         0.25\n",
      "  0.25       0.         0.25       0.         0.25      ]\n",
      " [0.         0.         0.         0.         0.2773501  0.2773501\n",
      "  0.2773501  0.2773501  0.2773501  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2773501  0.\n",
      "  0.         0.         0.2773501  0.         0.         0.\n",
      "  0.         0.2773501  0.         0.         0.         0.\n",
      "  0.2773501  0.         0.         0.         0.2773501  0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.2773501  0.2773501  0.         0.2773501  0.         0.\n",
      "  0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "TF-IDF з параметрами norm=None, use_idf=True, smooth_idf=False:\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         2.38629436 0.\n",
      "  0.         0.         2.38629436 2.38629436 0.         0.\n",
      "  2.38629436 0.         0.         2.38629436 0.         0.\n",
      "  0.         0.         0.         0.         2.38629436 0.\n",
      "  0.         0.         2.38629436 0.         0.         0.\n",
      "  0.         0.         0.         0.         2.38629436 0.\n",
      "  0.         0.         0.         0.         2.38629436 0.\n",
      "  0.         2.38629436 0.         2.38629436 0.        ]\n",
      " [2.38629436 0.         2.38629436 2.38629436 0.         0.\n",
      "  0.         0.         0.         2.38629436 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         2.38629436 0.         0.         0.         2.38629436\n",
      "  2.38629436 0.         2.38629436 2.38629436 0.         2.38629436\n",
      "  0.         0.         0.         0.         0.         2.38629436\n",
      "  0.         0.         0.         0.         0.         2.38629436\n",
      "  0.         0.         2.38629436 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         2.38629436 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         2.38629436\n",
      "  2.38629436 2.38629436 0.         0.         0.         2.38629436\n",
      "  0.         0.         0.         0.         2.38629436 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         2.38629436 0.         2.38629436 0.         0.\n",
      "  2.38629436 2.38629436 2.38629436 2.38629436 0.         0.\n",
      "  0.         0.         0.         0.         0.         2.38629436\n",
      "  2.38629436 0.         2.38629436 0.         2.38629436]\n",
      " [0.         0.         0.         0.         2.38629436 2.38629436\n",
      "  2.38629436 2.38629436 2.38629436 0.         0.         0.\n",
      "  0.         0.         0.         0.         2.38629436 0.\n",
      "  0.         0.         2.38629436 0.         0.         0.\n",
      "  0.         2.38629436 0.         0.         0.         0.\n",
      "  2.38629436 0.         0.         0.         2.38629436 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  2.38629436 2.38629436 0.         2.38629436 0.         0.\n",
      "  0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "TF-IDF з параметрами norm='l2', use_idf=False, smooth_idf=True:\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.30151134 0.\n",
      "  0.         0.         0.30151134 0.30151134 0.         0.\n",
      "  0.30151134 0.         0.         0.30151134 0.         0.\n",
      "  0.         0.         0.         0.         0.30151134 0.\n",
      "  0.         0.         0.30151134 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.30151134 0.\n",
      "  0.         0.         0.         0.         0.30151134 0.\n",
      "  0.         0.30151134 0.         0.30151134 0.        ]\n",
      " [0.2773501  0.         0.2773501  0.2773501  0.         0.\n",
      "  0.         0.         0.         0.2773501  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.2773501  0.         0.         0.         0.2773501\n",
      "  0.2773501  0.         0.2773501  0.2773501  0.         0.2773501\n",
      "  0.         0.         0.         0.         0.         0.2773501\n",
      "  0.         0.         0.         0.         0.         0.2773501\n",
      "  0.         0.         0.2773501  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.25       0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.25\n",
      "  0.25       0.25       0.         0.         0.         0.25\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.25       0.         0.25       0.         0.\n",
      "  0.25       0.25       0.25       0.25       0.         0.\n",
      "  0.         0.         0.         0.         0.         0.25\n",
      "  0.25       0.         0.25       0.         0.25      ]\n",
      " [0.         0.         0.         0.         0.2773501  0.2773501\n",
      "  0.2773501  0.2773501  0.2773501  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2773501  0.\n",
      "  0.         0.         0.2773501  0.         0.         0.\n",
      "  0.         0.2773501  0.         0.         0.         0.\n",
      "  0.2773501  0.         0.         0.         0.2773501  0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.2773501  0.2773501  0.         0.2773501  0.         0.\n",
      "  0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "def get_stats(fitted_transform):\n",
    "    tfidf_transformer1 = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True)\n",
    "    tfidf_transformer2 = TfidfTransformer(norm=None, use_idf=True, smooth_idf=False)\n",
    "    tfidf_transformer3 = TfidfTransformer(norm='l2', use_idf=False, smooth_idf=True)\n",
    "\n",
    "    tfidf1 = tfidf_transformer1.fit_transform(fitted_transform)\n",
    "    tfidf2 = tfidf_transformer2.fit_transform(fitted_transform)\n",
    "    tfidf3 = tfidf_transformer3.fit_transform(fitted_transform)\n",
    "\n",
    "    # Виведення результатів\n",
    "    print(\"TF-IDF з параметрами norm='l2', use_idf=True, smooth_idf=True:\")\n",
    "    print(tfidf1.toarray())\n",
    "    print(\"\\nTF-IDF з параметрами norm=None, use_idf=True, smooth_idf=False:\")\n",
    "    print(tfidf2.toarray())\n",
    "    print(\"\\nTF-IDF з параметрами norm='l2', use_idf=False, smooth_idf=True:\")\n",
    "    print(tfidf3.toarray())\n",
    "    \n",
    "    \n",
    "get_stats(bow_eng)\n",
    "get_stats(bow_ukr)\n",
    "get_stats(vec_bigram_eng)\n",
    "get_stats(vec_bigram_ukr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реалізовані в пакеті nltk.stem бібліотеки NLTK обробки природної мови. \n",
    "Наприклад, алгоритм стемінга Портера реалізовано в класі PorterStemmer модуля nltk.stem.porter, \n",
    "алгоритм стемінга \"сніжний ком\" - в модулі nltk.stem.snowball. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст: There's something about the simplicity of spaghetti that speaks to the heart. \n",
      "The way it twirls on the fork, the comforting aroma of tomato sauce simmering on the stove. It's a love affair that never fades. \n",
      "Whether topped with meatballs, fresh basil, or a sprinkle of Parmesan, spaghetti has a way of bringing people together, creating moments of warmth and connection around the dinner table.\n",
      "Стеми за допомогою алгоритму Портера: ['there', \"'s\", 'someth', 'about', 'the', 'simplic', 'of', 'spaghetti', 'that', 'speak', 'to', 'the', 'heart', '.', 'the', 'way', 'it', 'twirl', 'on', 'the', 'fork', ',', 'the', 'comfort', 'aroma', 'of', 'tomato', 'sauc', 'simmer', 'on', 'the', 'stove', '.', 'it', \"'s\", 'a', 'love', 'affair', 'that', 'never', 'fade', '.', 'whether', 'top', 'with', 'meatbal', ',', 'fresh', 'basil', ',', 'or', 'a', 'sprinkl', 'of', 'parmesan', ',', 'spaghetti', 'ha', 'a', 'way', 'of', 'bring', 'peopl', 'togeth', ',', 'creat', 'moment', 'of', 'warmth', 'and', 'connect', 'around', 'the', 'dinner', 'tabl', '.']\n",
      "Стеми за допомогою алгоритму 'сніжного кома': ['there', \"'s\", 'someth', 'about', 'the', 'simplic', 'of', 'spaghetti', 'that', 'speak', 'to', 'the', 'heart', '.', 'the', 'way', 'it', 'twirl', 'on', 'the', 'fork', ',', 'the', 'comfort', 'aroma', 'of', 'tomato', 'sauc', 'simmer', 'on', 'the', 'stove', '.', 'it', \"'s\", 'a', 'love', 'affair', 'that', 'never', 'fade', '.', 'whether', 'top', 'with', 'meatbal', ',', 'fresh', 'basil', ',', 'or', 'a', 'sprinkl', 'of', 'parmesan', ',', 'spaghetti', 'has', 'a', 'way', 'of', 'bring', 'peopl', 'togeth', ',', 'creat', 'moment', 'of', 'warmth', 'and', 'connect', 'around', 'the', 'dinner', 'tabl', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = orig_eng\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "porter_stems = [porter_stemmer.stem(word) for word in word_tokenize(text)]\n",
    "\n",
    "# Стемінг з використанням алгоритму \"сніжного кома\"\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "snowball_stems = [snowball_stemmer.stem(word) for word in word_tokenize(text)]\n",
    "\n",
    "# Виведення результатів\n",
    "print(\"Текст:\", text)\n",
    "print(\"Стеми за допомогою алгоритму Портера:\", porter_stems)\n",
    "print(\"Стеми за допомогою алгоритму 'сніжного кома':\", snowball_stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст: Луцькі луки - це особливий куточок природи, який зачаровує своєю красою та спокоєм. \n",
      "Розташовані вони на околицях міста Луцька, і завдяки своїй величезній площі створюють враження нескінченної просторовості. \n",
      "У цій природній зоні можна зустріти різноманітні види рослин та тварин, які розфарбовують луки в різнобарв'я кожної пори року. \n",
      "Прогулянки по луцьких луках стають справжнім відпочинком для душі та джерелом натхнення для творчості.\n",
      "\n",
      "Стеми за допомогою алгоритму Портера: ['луцькі', 'луки', '-', 'це', 'особливий', 'куточок', 'природи', ',', 'який', 'зачаровує', 'своєю', 'красою', 'та', 'спокоєм', '.', 'розташовані', 'вони', 'на', 'околицях', 'міста', 'луцька', ',', 'і', 'завдяки', 'своїй', 'величезній', 'площі', 'створюють', 'враження', 'нескінченної', 'просторовості', '.', 'у', 'цій', 'природній', 'зоні', 'можна', 'зустріти', 'різноманітні', 'види', 'рослин', 'та', 'тварин', ',', 'які', 'розфарбовують', 'луки', 'в', 'різнобарв', \"'\", 'я', 'кожної', 'пори', 'року', '.', 'прогулянки', 'по', 'луцьких', 'луках', 'стають', 'справжнім', 'відпочинком', 'для', 'душі', 'та', 'джерелом', 'натхнення', 'для', 'творчості', '.']\n",
      "Стеми за допомогою алгоритму 'сніжного кома': ['луцькі', 'лука', '-', 'це', 'особливия', 'куточок', 'природи,', 'який', 'зачаровує', 'своєть', 'краса', 'тот', 'спокоєм.', 'розташовані', 'вонь', 'на', 'околиць', 'міст', 'луцька,', 'і', 'завдяк', 'своїть', 'величезніть', 'площі', 'створюють', 'враженний', 'нескінченної', 'просторовості.', 'у', 'цій', 'природніть', 'зоні', 'можный', 'зустріть', 'різноманітні', 'видти', 'рослиный', 'тот', 'тварин,', 'які', 'розфарбовують', 'лука', 'в', \"різнобарв'ть\", 'кожної', 'пори', 'року.', 'прогулянка', 'по', 'луцький', 'лука', 'стають', 'справжнім', 'відпочинок', 'для', 'душі', 'тот', 'джерелом', 'натхненний', 'для', 'творчості.']\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "\n",
    "text = orig_ukr\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "porter_stems = [porter_stemmer.stem(word) for word in word_tokenize(text)]\n",
    "\n",
    "# Стемінг з використанням алгоритму \"сніжного кома\"\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "# Фільтруємо результати стемінгу, видаляючи символи кінця рядка та пробіли\n",
    "snowball_stems = [morph.parse(word)[0].normal_form for word in text.split()]\n",
    "\n",
    "# Виведення результатів\n",
    "print(\"Текст:\", text)\n",
    "print(\"Стеми за допомогою алгоритму Портера:\", porter_stems)\n",
    "print(\"Стеми за допомогою алгоритму 'сніжного кома':\", snowball_stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вилучити стоп-слова з текстів речень, використовуючи клас stopwords пакету  nltk.corpus.\n",
    "Для цього попередньо завантажити набір стоп-слів англійської мови шляхом виклику функції nltk.download('stopwords') бібліотеки NLTK, import nltk.\n",
    "Дослідити можливість вилучення стоп-слів з текстів українською мовою."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Англійське речення без стоп-слів: 's something simplicity spaghetti speaks heart . way twirls fork , comforting aroma tomato sauce simmering stove . 's love affair never fades . Whether topped meatballs , fresh basil , sprinkle Parmesan , spaghetti way bringing people together , creating moments warmth connection around dinner table .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Dmytro\n",
      "[nltk_data]     Sotnyk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words_english = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Токенізація та вилучення стоп-слів для англійського речення\n",
    "words_english = word_tokenize(orig_eng)\n",
    "filtered_sentence_english = [\n",
    "    word for word in words_english if word.lower() not in stop_words_english\n",
    "]\n",
    "\n",
    "# Виведення результату для англійської мови\n",
    "print(\"Англійське речення без стоп-слів:\", \" \".join(filtered_sentence_english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Українське речення без стоп-слів: Луцькі луки - особливий куточок природи , зачаровує красою спокоєм . Розташовані околицях міста Луцька , величезній площі створюють враження нескінченної просторовості . природній зоні зустріти різноманітні види рослин тварин , розфарбовують луки різнобарв ' пори . Прогулянки луцьких луках стають справжнім відпочинком душі джерелом натхнення творчості .\n"
     ]
    }
   ],
   "source": [
    "stop_words_ukrainian = set()\n",
    "\n",
    "with open(\"stopwords_ua_set.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        words = line.strip().split(\",\")  # Split line by commas if necessary\n",
    "        for word in words:\n",
    "            word = word.strip(\"{}' \")  # Strip additional characters and whitespace\n",
    "            stop_words_ukrainian.add(word)\n",
    "\n",
    "# Токенізація та вилучення стоп-слів для англійського речення\n",
    "words_ukrainian = word_tokenize(orig_ukr)\n",
    "filtered_sentence_ukrainian = [\n",
    "    word for word in words_ukrainian if word.lower() not in stop_words_ukrainian\n",
    "]\n",
    "\n",
    "# Виведення результату для англійської мови\n",
    "print(\"Українське речення без стоп-слів:\", \" \".join(filtered_sentence_ukrainian))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Частина 2\n",
    "### Побудова моделі класифікації текстових документів "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Обрати набір даних з розміченими текстовими документами. \n",
    "Можна скористатися набором Internet Movie Database (IMdb), який включає 50 000 текстів рецензій на фільми та відношення до них - позитивна або негативна рецензії (мітки класів).\n",
    "Задача тоді буде полягати у побудові прогнозу щодо віднесення нової рецензії до одного з класів - позитивна або негативна рецензія."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "\n",
    "# Завантаження набору даних\n",
    "categories = [\"alt.atheism\", \"soc.religion.christian\", \"comp.graphics\", \"sci.med\"]\n",
    "data = fetch_20newsgroups(\n",
    "    subset=\"all\",\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Очистити текстові дані: вилучити небажані символи, розмітку, привести всі букви до нижнього регістру та інше (за необхідності).\n",
    "\n",
    "### 3. Підготувати текстові документи для їх наступної подачі на вхід алгоритмів МН: \n",
    "- розбити набір на навчальну, валідаційну та тестову підмножини."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Очищення та приведення тексту до нижнього регістру\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "# Розбиття даних\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    data.data, data.target, test_size=0.4, random_state=42\n",
    ")\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Конфігурація та пошук гіперпараметрів для наївного баєсівського класифікатора\n",
    "pipeline_nb = Pipeline(\n",
    "    [(\"tfidf\", TfidfVectorizer(tokenizer=clean_text)), (\"classifier\", MultinomialNB())]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Навчити модель для класифікації текстових документів (одну на вибір):\n",
    "   - логістичної регресії,   \n",
    "   - наївну баєсівську,\n",
    "   - іншу.\n",
    "Можна використати клас TfidfVectorizer, який комбінує CountVectorizer та TfidfTransformer модуля sklearn.feature_extraction.text.\n",
    "Виконати решітчастий пошук гіперпараметрів моделі класифікації та моделей обробки текстів, таких як ngram_range, stop_words, tokenizer та ін.\n",
    "Достатньо задати лімітовану кількість комбінацій гіперпараметрів, оскільки велика кількість векторів ознак\n",
    "і великий глосарій призводять до надзвичайно довгого виконання решітчастого пошуку.\n",
    "Надрукувати результуючий найкращий набір гіперпараметрів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmytro Sotnyk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dmytro Sotnyk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найкращі параметри для наївного баєсівського класифікатора: {'classifier__alpha': 0.1, 'classifier__fit_prior': False, 'tfidf__max_df': 0.5, 'tfidf__min_df': 3, 'tfidf__ngram_range': (2, 3), 'tfidf__stop_words': 'english', 'tfidf__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "parameters_nb = {\n",
    "    \"tfidf__ngram_range\": [(1, 1), (1, 2), (2, 3)],\n",
    "    \"tfidf__stop_words\": [None, \"english\"],\n",
    "    \"tfidf__max_df\": [0.5, 0.75, 1.0],\n",
    "    \"tfidf__min_df\": [1, 2, 3],\n",
    "    \"tfidf__use_idf\": [True, False],\n",
    "    \"classifier__alpha\": [0.1, 1, 10],\n",
    "    \"classifier__fit_prior\": [True, False],\n",
    "}\n",
    "grid_search = GridSearchCV(pipeline_nb, parameters_nb, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\n",
    "    \"Найкращі параметри для наївного баєсівського класифікатора:\",\n",
    "    grid_search.best_params_,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Оцінити якість побудованої моделі на основі показника accuracy на навчальній, валідаційній та тестовій підмножинах. Зробити висновки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точність для підмножини: Тренувальної - 0.93, Валідаційної - 0.84, Тестової - 0.83\n"
     ]
    }
   ],
   "source": [
    "train_accuracy_nb = grid_search.score(X_train, y_train)\n",
    "valid_accuracy_nb = grid_search.score(X_valid, y_valid)\n",
    "test_accuracy_nb = grid_search.score(X_test, y_test)\n",
    "\n",
    "print(\n",
    "    f\"Точність для підмножини: Тренувальної - {train_accuracy_nb:.2f}, Валідаційної - {valid_accuracy_nb:.2f}, Тестової - {test_accuracy_nb:.2f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точність на валідаційній та тестовій підмножинах дещо нижча порівняно з тренувальною точністю. Це може вказувати на незначне перенавчання моделі, коли модель дуже добре вчиться на тренувальних даних, але її здатність до узагальнення на нових даних є трохи гіршою. Тим не менш, близькість валідаційної та тестової точностей є позитивним сигналом, оскільки це означає, що модель зберігає стабільність своїх показників на незалежних від тренування даних."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Для векторизації текстів в п.4 використати альтернативний клас HashingVectorizer з модуля sklearn.feature_extraction.text та метод word2vec.\n",
    "Модель HashingVectorizer дозволяє використовувати не всі вектори ознак навчального набору, задати кількість ознак n_features, та використовує трюк з хешуванням.\n",
    "Надзвичайно велика кількість ознак, які створюються при роботі з текстовими даними призводить до дуже великих обчислювальних витрат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Caching the list of root modules, please wait!\n",
      "(This will only be done once - type '%rehashx' to reset cache!)\n",
      "\n",
      "Найкращі параметри для наївного баєсівського класифікатора: {'vectorizer__n_features': 16384, 'vectorizer__norm': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmytro Sotnyk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def clean_text_hashing(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # видалення непотрібних символів\n",
    "    text = text.lower()  # перетворення до нижнього регістру\n",
    "    return text.split()  # повертає список слів\n",
    "\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"vectorizer\",\n",
    "            HashingVectorizer(\n",
    "                tokenizer=clean_text_hashing, n_features=2**10, alternate_sign=False\n",
    "            ),\n",
    "        ),  # Використання 1024 ознак і уникнення негативних значень\n",
    "        (\"classifier\", MultinomialNB()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    \"vectorizer__n_features\": [\n",
    "        2**10,\n",
    "        2**12,\n",
    "        2**14,\n",
    "        2**16,\n",
    "    ],  # Різні розміри векторного простору\n",
    "    \"vectorizer__norm\": [None, \"l1\", \"l2\"],  # Вид нормалізації\n",
    "}\n",
    "\n",
    "# Решітчастий пошук\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Виведення найкращих параметрів\n",
    "print(\n",
    "    \"Найкращі параметри для наївного баєсівського класифікатора:\",\n",
    "    grid_search.best_params_,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точність для підмножини: Тренувальної - 0.92, Валідаційної - 0.84, Тестової - 0.83\n"
     ]
    }
   ],
   "source": [
    "train_accuracy_nb = grid_search.score(X_train, y_train)\n",
    "valid_accuracy_nb = grid_search.score(X_valid, y_valid)\n",
    "test_accuracy_nb = grid_search.score(X_test, y_test)\n",
    "\n",
    "print(\n",
    "    f\"Точність для підмножини: Тренувальної - {train_accuracy_nb:.2f}, Валідаційної - {valid_accuracy_nb:.2f}, Тестової - {test_accuracy_nb:.2f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Word2VecVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.dim = 100  # Розмір векторів\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        sentences = [clean_text_hashing(text) for text in X]\n",
    "        self.model = Word2Vec(sentences, vector_size=self.dim, window=5, min_count=1, workers=4)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.model.wv[word] for word in clean_text_hashing(text) if word in self.model.wv]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for text in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;word2vec_vectorizer&#x27;, Word2VecVectorizer()),\n",
       "                (&#x27;classifier&#x27;, LogisticRegression(max_iter=1000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;word2vec_vectorizer&#x27;, Word2VecVectorizer()),\n",
       "                (&#x27;classifier&#x27;, LogisticRegression(max_iter=1000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Word2VecVectorizer</label><div class=\"sk-toggleable__content\"><pre>Word2VecVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('word2vec_vectorizer', Word2VecVectorizer()),\n",
       "                ('classifier', LogisticRegression(max_iter=1000))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"word2vec_vectorizer\", Word2VecVectorizer()),  # векторизатор на базі word2vec\n",
    "        (\"classifier\", LogisticRegression(max_iter=1000)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    \"classifier__C\": [0.01, 0.1, 1, 10, 100], \n",
    "}\n",
    "\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точність для підмножини: Тренувальної - 0.92, Валідаційної - 0.84, Тестової - 0.83\n"
     ]
    }
   ],
   "source": [
    "train_accuracy_w2v = pipeline.score(X_train, y_train)\n",
    "valid_accuracy_w2v = pipeline.score(X_valid, y_valid)\n",
    "test_accuracy_w2v = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\n",
    "    f\"Точність для підмножини: Тренувальної - {train_accuracy_nb:.2f}, Валідаційної - {valid_accuracy_nb:.2f}, Тестової - {test_accuracy_nb:.2f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Частина 3\n",
    "### Основи тематичного моделювання (topic modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Обрати набір даних з нерозміченими (!) текстовими документами. Або скористатися набором Internet Movie Database (IMdb) без врахування міток класів. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Завантаження набору даних\n",
    "data = fetch_20newsgroups(\n",
    "    subset=\"all\",\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    ")\n",
    "\n",
    "# Тексти для обробки\n",
    "text_data = data.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Виконати векторизацію текстів - побудувати вектори числових ознак для речень, використовуючи:\n",
    "- модель bag-of-words (підсумовування слів) або word2vec,\n",
    "\n",
    "та клас CountVectorizer модуля sklearn.feature_extraction.text або іншу модель векторизації.\n",
    "\n",
    "Встановити значення наступних параметрів класу CountVectorizer:  stop_words,  max_df,  max_features.\n",
    "Встановлення max_df, наприклад 0.1, дозволяє виключити з подальшого аналізу слова, які занадто часто зустрічаються в документах, є загальновживаними словами.\n",
    "Параметр max_features задає обмеження на кількість слів, які будуть враховуватися. Це буде кількість слів, які зустрічаються найчастіше.  Наприклад, розмірність набору даних можна визначити max_features = 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Розміри матриці термінів-документів: (18846, 1033)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    stop_words=\"english\",  # виключення слів, які зустрічаються дуже часто\n",
    "    max_df=0.05, # ігнорування термінів, які зустрічаються у більш ніж 5% документів\n",
    "    min_df=0.01,  \n",
    "    max_features=5000,  # обмеження вектора 5000 найбільш частими словами\n",
    ")\n",
    "\n",
    "# Обробка текстів та створення матриці термінів-документів\n",
    "X = vectorizer.fit_transform(text_data)\n",
    "\n",
    "print(\"Розміри матриці термінів-документів:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Виконати розбиття текстів документів на теми, використовуючи латентне розміщення Дирихле (LDA),яке приймає на вхід матрицю підсумовування слів.\n",
    "Використати клас LatentDirichletAllocation з sklearn.decomposition або іншу реалізацію.\n",
    "Задати n_components - кількість тем.\n",
    "Розглянути різні значення параметру learning_method класу LatentDirichletAllocation:\n",
    "- learning_method = ‘batch’ - оцінювання на основі всіх доступних навчальних даних - на всій матриці підсумовування слів в одній ітерації,\n",
    "- learning_method = ‘online’ - міні-пакетне або динамічне навчання."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(learning_method=&#x27;online&#x27;, n_components=5,\n",
       "                          random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(learning_method=&#x27;online&#x27;, n_components=5,\n",
       "                          random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(learning_method='online', n_components=5,\n",
       "                          random_state=42)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=5, random_state=42, learning_method='batch')\n",
    "lda.fit(X)\n",
    "\n",
    "lda_online = LatentDirichletAllocation(n_components=5, random_state=42, learning_method='online')\n",
    "lda_online.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Після навчання надрукувати десять найбільш важливих слів для кожної теми."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тема #0:\n",
      "jesus life children mr evidence state person law government mean\n",
      "Тема #1:\n",
      "key game government team gun games chip encryption play clipper\n",
      "Тема #2:\n",
      "00 25 20 15 12 11 14 16 17 50\n",
      "Тема #3:\n",
      "drive card car disk scsi hard mac speed memory pc\n",
      "Тема #4:\n",
      "file program space image data files ftp dos software version\n"
     ]
    }
   ],
   "source": [
    "# Виведення ключових слів для кожної теми\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Тема #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тема #0:\n",
      "government space university president public national research state general article\n",
      "Тема #1:\n",
      "drive dos card car disk mac pc hard price scsi\n",
      "Тема #2:\n",
      "jesus life mean evidence reason wrong gun person says man\n",
      "Тема #3:\n",
      "00 25 game 12 15 20 11 16 14 team\n",
      "Тема #4:\n",
      "file program key version files image ftp list data graphics\n"
     ]
    }
   ],
   "source": [
    "# Виведення ключових слів для кожної теми\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda_online.components_):\n",
    "    print(\"Тема #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Здогадатися і написати назви тем, які розпізнав алгоритм LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA\n",
    "- \"talk.politics.misc\"\n",
    "- \"rec.sport.baseball\" \n",
    "- \"misc.forsale\"\n",
    "- \"comp.sys.mac.hardware\"\n",
    "- \"comp.os.ms-windows.misc\"\n",
    "\n",
    "### LDA-ONLINE\n",
    "- \"sci.space\"\n",
    "- \"comp.sys.ibm.pc.hardware\"\n",
    "- \"talk.religion.misc\"\n",
    "- \"misc.forsale\"\n",
    "- \"comp.os.ms-windows.misc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Поекспериментувати з різними значеннями n_components кількість тем.\n",
    "Зробити висновки.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тема #0:\n",
      "jesus israel christian bible jews life christ church religion christians\n",
      "Тема #1:\n",
      "window key application line program running value sound manager mouse\n",
      "Тема #2:\n",
      "00 25 12 20 15 11 14 17 16 13\n",
      "Тема #3:\n",
      "image file color files program format display screen version images\n",
      "Тема #4:\n",
      "file space ftp list data send program server software graphics\n",
      "Тема #5:\n",
      "car went told left heard children bike started saw came\n",
      "Тема #6:\n",
      "drive dos card scsi disk price pc hard mac monitor\n",
      "Тема #7:\n",
      "government law president gun mr public key state encryption security\n",
      "Тема #8:\n",
      "game team games play season hockey win players league won\n",
      "Тема #9:\n",
      "book university research health study medical books science article high\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=10, random_state=42, learning_method='batch')\n",
    "lda.fit(X)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Тема #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тема #0:\n",
      "israel jews life christian religion jewish human christians religious person\n",
      "Тема #1:\n",
      "war gun government control window 000 population million crime event\n",
      "Тема #2:\n",
      "25 12 11 14 15 20 17 16 18 13\n",
      "Тема #3:\n",
      "image images color data format file display files program 24\n",
      "Тема #4:\n",
      "ftp list server graphics software version faq sun send unix\n",
      "Тема #5:\n",
      "children went told started saw killed came women left home\n",
      "Тема #6:\n",
      "drive 00 scsi disk hard sale drives controller 50 price\n",
      "Тема #7:\n",
      "law state government rights police gun states guns laws public\n",
      "Тема #8:\n",
      "card mac video apple monitor memory pc driver drivers mode\n",
      "Тема #9:\n",
      "book church word books paul jesus bible words theory christ\n",
      "Тема #10:\n",
      "game team games play season hockey players league win player\n",
      "Тема #11:\n",
      "jesus lord hell father son says christ modem shall hear\n",
      "Тема #12:\n",
      "car bike buy stuff speed high money big engine pretty\n",
      "Тема #13:\n",
      "address phone info send email data cs current box computer\n",
      "Тема #14:\n",
      "key chip keys clipper encryption public government algorithm message private\n",
      "Тема #15:\n",
      "space research university health nasa medical center 1993 earth april\n",
      "Тема #16:\n",
      "technology ms press national government security president development today white\n",
      "Тема #17:\n",
      "mr president yes mean clinton general statement decision talk kind\n",
      "Тема #18:\n",
      "hi files advance appreciated anybody copy machine net program interested\n",
      "Тема #19:\n",
      "file dos program error window output command line open manager\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=20, random_state=42, learning_method='batch')\n",
    "lda.fit(X)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Тема #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зі збільшенням кількості компонентів (тем) з 10 до 20, спостерігається більш фінікова роздільність субтем. Наприклад, великі теми, що стосуються релігії та комп'ютерних технологій, розбиваються на більш специфічні теми. \n",
    "При більшій кількості тем зростає ризик перекриття тем, коли деякі теми можуть здатися надто схожими одна на одну. Наприклад, можуть бути окремі теми для різних аспектів технологій, які могли б бути об'єднані в одну при меншій кількості тем."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
