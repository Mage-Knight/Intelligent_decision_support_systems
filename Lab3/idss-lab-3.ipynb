{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":755761,"sourceType":"datasetVersion","datasetId":392005}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Лабораторна робота 3 з ІСППР (варіант 22)\n### Виконали студенти групи КІ-31мп Шабо О.А. та Сотник Д.C.\n##### Датасет: https://www.kaggle.com/datasets/rahulsah06/gooogle-stock-price","metadata":{"execution":{"iopub.execute_input":"2024-03-16T15:41:14.758101Z","iopub.status.busy":"2024-03-16T15:41:14.757649Z","iopub.status.idle":"2024-03-16T15:41:14.795097Z","shell.execute_reply":"2024-03-16T15:41:14.794094Z","shell.execute_reply.started":"2024-03-16T15:41:14.758066Z"}}},{"cell_type":"code","source":"import torch\nimport random\nimport gc\nimport os\n\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport pandas as pd\nimport numpy as np\nimport itertools as it\nimport matplotlib.pyplot as plt\nimport itertools as it\n\nimport torch.optim as optim\nimport torch.utils.data as data\nimport sklearn.preprocessing as skp\n\nfrom tqdm.notebook import tqdm\nfrom copy import deepcopy\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-16T19:33:21.047625Z","iopub.status.busy":"2024-03-16T19:33:21.047235Z","iopub.status.idle":"2024-03-16T19:33:21.055651Z","shell.execute_reply":"2024-03-16T19:33:21.054265Z","shell.execute_reply.started":"2024-03-16T19:33:21.047595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    print(\"PyTorch GPU is available\")\n    DEVICE = \"cuda\"\nelse:\n    print(\"PyTorch GPU is not available\")\n    DEVICE = \"cpu\"\n    \nBATCH_SIZE = 16\nGLOBAL_OPTIMIZER = lambda nn_parameters: optim.Adam(nn_parameters, lr=0.01, weight_decay=1e-4)\nGLOBAL_SCHEDULER = lambda optimizer: optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=7, factor=0.8, min_lr=2e-8, mode=\"min\", verbose=False)\nGLOBAL_CRITERION = lambda: nn.MSELoss().to(DEVICE)\nLAB_THREE_DIR = \"/kaggle/working\"\nFEATURE_NAMES = ['Open', 'High', 'Low', 'Volume']\nTARGET_NAME = 'Close'\nALL_VARIABLE_NAMES = FEATURE_NAMES + list(TARGET_NAMES)\nIN_QUANT = 14\nOUT_QUANT = 7","metadata":{"execution":{"iopub.execute_input":"2024-03-16T19:33:21.058940Z","iopub.status.busy":"2024-03-16T19:33:21.058473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reproducibility","metadata":{}},{"cell_type":"code","source":"RANDOM_SEED = 10\n\n# Seed the RNG for all devices (both CPU and CUDA)\ntorch.manual_seed(RANDOM_SEED)\n# Set python seed\nrandom.seed(RANDOM_SEED)\n# Set numpy seed\nnp.random.seed(RANDOM_SEED)\n\ntorch.cuda.manual_seed_all(RANDOM_SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n\n# Worker initialization function for data loaders (simplest approach)\ndef seed_worker(worker_id):\n    worker_seed = (torch.initial_seed() + worker_id) % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng_train = torch.Generator().manual_seed(RANDOM_SEED)\ng_test = torch.Generator().manual_seed(RANDOM_SEED+1)","metadata":{"execution":{"iopub.execute_input":"2024-03-16T19:33:21.070346Z","iopub.status.busy":"2024-03-16T19:33:21.069357Z","iopub.status.idle":"2024-03-16T19:33:21.081496Z","shell.execute_reply":"2024-03-16T19:33:21.080340Z","shell.execute_reply.started":"2024-03-16T19:33:21.070314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/gooogle-stock-price/Google_Stock_Price_Train.csv')\ndf_train.tail()","metadata":{"execution":{"iopub.execute_input":"2024-03-16T19:33:21.083943Z","iopub.status.busy":"2024-03-16T19:33:21.083548Z","iopub.status.idle":"2024-03-16T19:33:21.112462Z","shell.execute_reply":"2024-03-16T19:33:21.111367Z","shell.execute_reply.started":"2024-03-16T19:33:21.083914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/gooogle-stock-price/Google_Stock_Price_Test.csv')\ndf_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Close'] = pd.to_numeric(df_train['Close'].str.replace(',', ''), errors='coerce')\ndf = pd.concat([df_train, df_test], axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data split and normalization","metadata":{}},{"cell_type":"code","source":"# Select features and target for modeling\ndf['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y')\ndf['Volume'] = pd.to_numeric(df['Volume'].str.replace(',', ''), errors='coerce')\n\nfeatures = df[['Open', 'High', 'Low', 'Volume']]\ntarget = df['Close'].to_frame()\nfull_df = df[['Open', 'High', 'Low', 'Volume','Close']]\n\n# Normalize features\nfeature_scaler = StandardScaler()\ntarget_scaler = deepcopy(feature_scaler)\nfull_scaler = deepcopy(feature_scaler)\nsplit_index = int(len(df) * 0.75)\nfeature_scaler.fit(features.head(split_index)) # we have access only to train set\ntarget_scaler.fit(target.head(split_index))\nfull_scaler.fit(full_df.head(split_index))\nfull_df_normalized = pd.DataFrame(full_scaler.transform(full_df), col=full_df.columns)\n\n# Split the data\ndf_train_norm = full_df_normalized.iloc[:split_index]\ndf_test_norm = full_df_normalized.iloc[split_index:]\ndf_train = full_df.iloc[:split_index]\ndf_test = full_df.iloc[split_index:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TimeSeriesDataset(data.TensorDataset):\n    \"\"\" Class that creates dataset for timeseries \"\"\"\n    def __init__(self, data, in_data_size, out_data_size, used_feat=ALL_VARIABLE_NAMES,\n                 target_feat=TARGET_NAME):\n        self.in_data_size = in_data_size\n        self.out_data_size = out_data_size\n        self.inputs, self.outputs = TimeSeriesDataset.to_supervised(data, in_data_size, out_data_size,\n                                                                used_feat, target_feat)\n        super(TimeSeriesDataset, self).__init__(self.inputs, self.outputs)\n        \n    def to_supervised(data, in_data_size, out_data_size, used_feat, target_feat):\n        \"\"\"\n        Transform timeseries to a supervised problem.\n        Parameters:\n            - data: numpy.ndarray\n                The input data of shape (num_records, num_features).\n            - in_data_size: int\n                The size of the input data window.\n            - out_data_size: int\n                The size of the output data window.\n        \"\"\"\n        X, y = [], []\n        for i in range(len(data)-in_data_size-out_data_size+1):\n            feature = data[used_feat].iloc[i:i+in_data_size]\n            target = data[target_feat].iloc[i+in_data_size:i+in_data_size+out_data_size]\n            X.append(feature.values)\n            y.append(target.values)\n        X = torch.tensor(np.array(X)).view(len(data)-in_data_size-out_data_size+1, in_data_size, data.shape[1])\n        y = torch.tensor(np.array(y)).view(len(data)-in_data_size-out_data_size+1, out_data_size, 1)\n        \n#         assert X.shape == (len(data)-in_data_size-out_data_size+1, in_data_size, data.shape[1])\n#         assert y.shape == (len(data)-in_data_size-out_data_size+1, out_data_size, 1)\n        return X, y\n            \n    \n    def __len__(self):\n        assert self.inputs.shape[0] == self.outputs.shape[0]\n        return self.inputs.shape[0]\n\n    def __getitem__(self, idx)\n        return self.inputs[idx], self.outputs[idx]\n    \ntrain_set_norm = TimeSeriesDataset(df_train_norm, IN_QUANT, OUT_QUANT)\ntest_set_norm = TimeSeriesDataset(pd.concatenate((df_train_norm.iloc[-IN_QUANT:],df_test_norm), axis=1),\n                             IN_QUANT, OUT_QUANT)\ntest_set = TimeSeriesDataset(pd.concatenate((df_train.iloc[-IN_QUANT:],df_test), axis=1),\n                             IN_QUANT, OUT_QUANT)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Backprop training","metadata":{}},{"cell_type":"code","source":"class BackpropTraining:\n    def __init__(\n        self,\n        nnetwork,\n        scheduler,\n        criterion,\n        optimizer,\n        train_set_norm,\n        test_set_norm,\n        test_set,\n        feature_scaler=feature_scaler,\n        target_scaler=target_scaler,\n        device=DEVICE,\n        checkpoint_dir=None,\n        global_verbosity=True\n    ):\n        \"\"\"\n        Backpropagation training abstraction class.\n        \n        Parameters\n        ----------\n        nnetwork :\n            CNN architecture model, already created and sent to device\n        optimizer :\n            Already instantiated optimizer\n        criterion :\n            Already instantiated criterion\n        train_set :\n            Training dataset of type TimeSeriesDataset\n        test_set :\n            Test dataset of type TimeSeriesDataset\n        device :\n            Device where data will be sent to (for example cuda)\n        checkpoint_dir : optional\n            Path to directory where checkpoint state lies\n        global_verbosity : optional\n            Defines how many messages should be displayed on a glonal class level\n        \"\"\"\n        \n        self.optimizer = optimizer\n        self.criterion = criterion\n        self.scheduler = scheduler\n        self.train_set_norm = train_set_norm # normalized\n        self.test_set_norm = test_set_norm # normalized\n        self.test_set = test_set\n        self.feature_scaler = feature_scaler,\n        self.target_scaler = target_scaler\n        self.device = device\n        self.nnetwork = nnetwork\n        self.global_verbosity = global_verbosity\n        self.arr_loaded = False # whether we loaded loss arrays\n \n        self.train_loader = data.DataLoader(train_set_norm, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=0,\n            worker_init_fn=seed_worker, generator=g_train)\n        self.test_loader = data.DataLoader(test_set_norm, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=0,\n            worker_init_fn=seed_worker, generator=g_test)\n        \n        self.train_batches_num = len(self.train_loader)\n        self.test_batches_num = len(self.test_loader)\n        \n        # Load training model from checkpoint\n        if checkpoint_dir is not None:\n            if self.global_verbosity:\n                print(\"Loading BackpropTraining model from checkpoint...\")\n            checkpoint = torch.load(checkpoint_dir)\n            self.nnetwork.load_state_dict(checkpoint['nnetwork_param'])\n            self.scheduler.load_state_dict(checkpoint['scheduler_state'])\n            self.start_epoch = checkpoint['current_epoch'] + 1\n            self.best_rmse = checkpoint['best_rmse']\n            \n            # Load rmse arrays\n            self.train_rmse_arr = checkpoint['train_mse_arr']\n            self.val_rmse_arr = checkpoint['val_rmse_arr']\n            self.val_rmse_vec_arr = checkpoint['val_rmse_vec_arr']\n            self.arr_loaded = True\n            \n            # Load environment state\n            torch.set_rng_state(checkpoint['rng_state'])\n            torch.cuda.set_rng_state(checkpoint['cuda_rng_state'])\n            np.random.set_state(checkpoint['np_random_state'])\n            random.setstate(checkpoint['random_state'])\n            if self.global_verbosity:\n                print(f\"!This checkpoint trained for {self.start_epoch} epochs, \"\n                      f\"current start_epoch={self.start_epoch + 1}!\")\n        # Create blank training model\n        else:\n            if self.global_verbosity:\n                print(\"Creating BackpropTraining model from scratch...\")\n            self.start_epoch = 0\n            self.best_rmse= np.inf\n        if self.global_verbosity:\n            print(\"Model successfully created!\")\n        self.checkpoint_directory = f'{type(self.nnetwork).__name__}_checkpoints'\n        \n    def train_validate_cycle(self, epochs, cycle_epoch=1, cycle_batch=9, verbosity=False):\n        \"\"\"\n        Performs training with the use of optimizer and learning rate scheduler.\n        \n        Parameters\n        ----------\n        epochs :\n            Number of training and validation cycle epochs\n        cycle_epoch : optional\n            The number that determines after which amount of epochs information will be displayed\n        cycle_batch : optional\n            The number that determines after which amount of batches in each epoch information will\n            be displayed\n        verbosity : optional\n            Whether messages are displayed\n        \"\"\"    \n        # Define flag for info display\n        print_ind = False\n\n        # Create required arrays to store rmse\n        if not self.arr_loaded:\n            self.train_rmse_arr = []\n            self.val_rmse_arr = []\n            self.val_rmse_vec_arr = []\n        self.epochs = epochs\n        \n        ##### Training for epochs #####\n        # Create training and validation loop\n        if self.global_verbosity:\n            print(f\"Training model for {epochs} epochs from {self.start_epoch + 1} to\"\n                  f\" {self.start_epoch + epochs}.\")\n            global_progress_bar = tqdm(range(self.start_epoch, self.start_epoch + epochs))\n        else:\n            global_progress_bar = range(self.start_epoch, self.start_epoch + epochs)\n            \n        for epoch in global_progress_bar:\n            output_epoch_flag = (epoch + 1) % cycle_epoch == 0 and self.global_verbosity\n            if output_epoch_flag:\n                print(f\"Current epoch: {epoch + 1}\\n-------\")\n                print_ind = True\n\n            ##### Training #####\n            # Set network to train mode\n            self.nnetwork.train()\n\n            if not output_epoch_flag:\n                progress_bar = enumerate(self.train_loader)\n            else:\n                progress_bar = tqdm(enumerate(self.train_loader))\n\n            # Add a loop through training batches    \n            for batch_num, (tr_inputs, real_tr_outputs) in progress_bar:\n                # Send data to cuda (preferably)\n                tr_inputs = tr_inputs.to(self.device)\n                real_tr_outputs = real_tr_outputs.to(self.device)\n\n                # Zero the parameter gradients\n                self.optimizer.zero_grad()\n\n                # Forward, backward pass + optimization\n                pred_tr_outputs = self.nnetwork(tr_inputs).to(self.device)\n                loss = self.criterion(pred_tr_outputs, real_tr_outputs)\n                loss.backward()\n                self.optimizer.step()\n\n                # Print out how many samples have been seen\n                # Output loss each cycle batches (cycle*batch samples)\n                if verbosity and (batch_num + 1) % cycle_batch == 0 and batch_num != 0 and print_ind:\n                    print(f\"Looked at {batch_num * BATCH_SIZE} samples\")\n                    print(f\"Current train loss for batch: {loss}\")\n\n                # Clearing memory\n                del tr_inputs\n                del real_tr_outputs\n                del pred_tr_outputs\n            ##### Training end #####\n\n            ##### Validation #####\n            # Set network to evaluation mode\n            self.nnetwork.eval()\n            # Turn on inference mode (no autograd)\n            with torch.inference_mode():\n                average_rmse_train = self.evaluate_rmse(self.train_loader)\n                average_rmse_test, average_rmse_test_vec =\\\n                    self.evaluate_rmse(self.test_loader, return_rmse_vec=True)  \n            ##### Validation end #####\n                \n                # Modify learning rate based on metric value\n                self.scheduler.step(average_rmse_test)\n                # Save average (throughout the epoch) rmse\n                self.train_rmse_arr.append(average_rmse_train)\n                self.val_rmse_arr.append(average_rmse_test)\n                self.val_rmse_vec_arr.append(average_rmse_test_vec)\n                \n                # Check if length of arrays is appropriate\n                assert len(self.train_rmse_arr)==len(self.val_rmse_arr)==len(self.val_rmse_vec_arr)\n                assert epoch + 1 == len(self.train_rmse_arr)\n                \n                if output_epoch_flag:\n                    # Print out average rmse of this epoch each cycle_epoch\n                    print(f\"\\nAverage train rmse: {self.train_rmse_arr[epoch]:.5f}\")\n                    print(f\"Average validation rmse: {self.val_rmse_arr[epoch]:.5f}\")\n                    print_ind = False\n                    \n                # Save checkpoint in case of better rmse on validation set\n                if self.best_rmse > self.val_rmse_arr[epoch]:\n                    if output_epoch_flag:\n                        print(f'\\nSaving {epoch+1} status:\\'best\\' checkpoint, '\n                              f'current average rmse: {self.val_rmse_arr[epoch]:.5f} is '\n                              f'better than previous best result: {self.best_rmse:.5f}')\n                    self.best_rmse = self.val_rmse_arr[epoch]\n                    self.checkpoint(epoch, 'best', output_epoch_flag)\n                else:\n                    if output_epoch_flag:\n                        print(f'\\nSaving {epoch+1} status:\\'last\\' checkpoint, '\n                              f'current average rmse: {self.val_rmse_arr[epoch]:.5f}')\n                    self.checkpoint(epoch, 'last', output_epoch_flag)\n                    \n            # Garbage collection     \n            gc.collect()\n            torch.cuda.empty_cache()\n        ##### Training for epochs END #####\n        self.start_epoch += epochs # Save current epoch to start new training from it\n    \n    def checkpoint(self, current_epoch, status, verbosity, directory_name=None):\n        \"\"\" Save current_epoch checkpoint \"\"\"\n        state = {\n            'nnetwork_param': self.nnetwork.state_dict(),\n            'scheduler_state': self.scheduler.state_dict(),\n            'current_epoch': current_epoch,\n            'best_rmse': self.best_rmse,\n            'train_rmse_arr': self.train_rmse_arr,\n            'val_rmse_arr': self.val_rmse_arr,\n            'val_rmse_arr_vec': self.val_rmse_arr_vec,\n            'rng_state': torch.get_rng_state(),\n            'cuda_rng_state':torch.cuda.get_rng_state(),\n            'np_random_state': np.random.get_state(), \n            'random_state': random.getstate()\n        }\n\n        if directory_name is None:\n            if not os.path.isdir(self.checkpoint_directory):\n                os.mkdir(self.checkpoint_directory)\n\n            torch.save(state, './'+ self.checkpoint_directory +\\\n                        f'/epoch_{status}_cpt')\n        else:\n            if not os.path.isdir(directory_name):\n                os.mkdir(directory_name)\n            torch.save(state, './'+ directory_name + '/grid_search_best_cpt')\n            \n        if verbosity:\n            print(f'Successfully saved epoch {current_epoch + 1} status:{status} checkpoint!\\n')\n    \n    def test_and_plot(self, df_full=full_df, split_index=split_index, pred_feature=PRED_IDX):\n        \"\"\" Perform test by forecasting initial data \"\"\"\n        def calculate_rmse_vec(predicted_values, real_values):\n            # Calculate the squared difference between predicted and real values\n            squared_diff = (predicted_values - real_values) ** 2\n            # Calculate the mean squared difference along the coordinate dimension\n            mean_squared_diff = torch.mean(squared_diff, dim=1)\n            # Calculate the root mean squared difference\n            rmse_vec = torch.sqrt(mean_squared_diff)\n            return rmse_vec\n        \n        def apply_scaler_to_multidim_numpy(numpy_array, target_scaler):\n            scaled_arr = numpy_array.copy()\n            for i in range(scaled_arr.shape[1]):\n                scaled_arr[:, i] = target_scaler.inverse_transform(scaled_arr[:, i].reshape(-1, 1)).flatten()\n            return scaled_arr\n        \n        self.nnetwork.eval()\n        # Turn on inference mode (no autograd)\n        with torch.inference_mode():\n            pred_train_outputs = apply_scaler_to_multidim_numpy(self.nnetwork(self.train_set_norm.inputs.to(self.device)).cpu().numpy(), self.target_scaler)\n            real_train_outputs = apply_scaler_to_multidim_numpy(self.train_set_norm.outputs, self.target_scaler)\n            pred_test_outputs = apply_scaler_to_multidim_numpy(self.nnetwork(self.test_set_norm.inputs.to(self.device)).cpu().numpy(), self.target_scaler)\n            real_test_outputs = self.test_set.outputs\n\n            rmse_train = np.sqrt(self.criterion(torch.tensor(pred_train_outputs).to(self.device),\\\n                torch.tensor(real_train_outputs).to(self.device)).item())\n            rmse_test = np.sqrt(self.criterion(torch.tensor(pred_test_outputs).to(self.device),\\\n                torch.tensor(real_test_outputs).to(self.device)).item())\n            rmse_test_vec = calculate_rmse_vec(pred_test_outputs, real_test_outputs)\n            \n            del pred_train_outputs\n            del real_train_outputs           \n            del pred_test_outputs\n            del real_test_outputs\n\n            ##### PLOT #####\n            # Define the x-axis values\n            assert self.train_set_norm.in_data_size == self.test_set_norm.in_data_size == self.test_set.in_data_size\n            assert self.train_set_norm.out_data_size == self.test_set_norm.out_data_size == self.test_set.out_data_size\n            used_steps = self.test_set.in_data_size\n            predicted_steps = self.test_set.out_data_size\n            # ensure the interval calculation logic is correct\n            assert split_index == used_steps + len(self.train_set_norm) + predicted_steps - 1\n            assert len(df_full[\"Date\"].values) == split_index + len(self.test_set_norm) +\\\n                predicted_steps - 1\n            # date interval where we make prediction on training set\n            x_train = [df_full[\"Date\"].values[used_steps+i:used_steps\\\n                        +len(self.train_set_norm)+i] for i in range(predicted_steps)]\n            # date interval where we make prediction on test set\n            x_test = [df_full[\"Date\"].values[split_index+i:split_index+len(self.test_set_norm)+i]\\\n                      for i in range(predicted_steps)]\n\n            # Define the number of columns and rows for subplots\n            num_cols = 3\n            num_rows = (predicted_steps + num_cols - 1) // num_cols #  \"+ num_cols - 1\" adds additional row for spare plots\n\n            # Create subplots with the specified number of rows and columns\n            fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 5*num_rows))\n            # Plot real_outputs from start to end\n            axs[row, col].plot(df[\"Date\"].values, full_df[pred_feature].values, label=\"Real Outputs\", color=\"black\")\n            \n            # Iterate over each predicted step\n            for k in range(predicted_steps):\n                # Calculate the row and column index for the current subplot\n                row = k // num_cols\n                col = k % num_cols\n\n                # Get the k-th column from pred_train_outputs, pred_test_outputs, and real_outputs\n                pred_train = pred_train_outputs[:, k]\n                pred_test = pred_test_outputs[:, k]\n\n                # Plot pred_train_outputs starting from x=USED_QUANT+k on the x-axis\n                axs[row, col].plot(x_train[k], pred_train, label=f\"Predicted Train Outputs for {k+1}-th step\", color=\"red\")\n\n                # Plot pred_test_outputs starting from x=pred_train_outputs.shape[0]+k on the x-axis\n                axs[row, col].plot(x_test[k], pred_test, label=f\"Predicted Test Outputs for {k+1}-th step\", color=\"blue\")\n\n                if k >= predicted_steps - num_cols:\n                    # Set the x-axis ticks\n                    axs[row, col].set_xticks(df[\"Date\"].values[::10])\n                    axs[row, col].set_xticklabels(df[\"Date\"].values[::10], rotation=45)\n\n                # Add a legend for each plot\n                axs[row, col].legend()\n\n            # Show the plots\n            plt.show()\n            ##### PLOT END #####\n            \n        gc.collect()\n        torch.cuda.empty_cache()\n        return rmse_train, rmse_test, rmse_test_vec\n    \n    def evaluate_rmse(self, dataloader, return_rmse_vec=False):\n        \"\"\" Evaluate model performance on data from dataloader \"\"\"\n        mse = 0\n        mse_vec = np.zeros(self.test_set.out_data_size)\n        with torch.inference_mode():\n            for inputs, real_outputs in dataloader:\n                # Send data to cuda (preferably)\n                inputs = inputs.to(self.device)\n                real_outputs = real_outputs.to(self.device)\n                pred_outputs = self.nnetwork(inputs)\n                # Test loss and accuracy calculation\n                current_mse_vec = self.criterion(pred_outputs, real_outputs).numpy()\n                mse_vec += current_mse_vec\n                mse += np.mean(current_mse_vec)\n\n                # Clearing memory\n                del inputs\n                del real_outputs\n                del pred_outputs\n        if not return_rmse_vec: \n            return np.sqrt(mse / len(dataloader))\n        else:\n            return np.sqrt(mse / len(dataloader)), np.sqrt(mse_vec / len(dataloader))\n    \n    def plot_metrics(self):\n        \"\"\"\n        Plot rmse plots for train and validations sets.\n        Works with latest epochs, train and val rmse arrays respectively.\n        \"\"\"\n        plt.figure(figsize = (15, 5))\n        epoch_arr = np.arange(1, len(self.train_rmse_arr) + 1)\n        plt.plot(epoch_arr, self.train_rmse_arr, 'r', label='Average train rmse')\n        plt.plot(epoch_arr, self.val_rmse_arr,'b', label='Average validation rmse')\n        plt.title(\"Average rmse\")\n        plt.legend()\n        plt.xlabel(\"Epoch number\")\n        plt.ylabel(\"RMSE value\")\n        plt.show()\n\n    @staticmethod\n    def find_best_model(used_timesteps_arr, train_data_norm, test_data_norm, test_data,\n                        nnetwork_template, epochs, used_feat, target_feat, scheduler_creator=GLOBAL_SCHEDULER,\n                        criterion=GLOBAL_CRITERION(), optimizer_creator=GLOBAL_OPTIMIZER):\n        \"\"\" Find the best amount of previous steps used for prediction based on the average RMSE on the test set\"\"\"\n        best_prev_steps_used = None\n        best_average_rmse_test = float('inf')\n\n        for in_timesteps in tqdm(used_timesteps_arr):\n            train_set_norm = TimeSeriesDataset(train_data_norm, in_timesteps, OUT_QUANT, used_feat, target_feat)\n            test_set_norm = TimeSeriesDataset(pd.concatenate((train_data_norm.iloc[-in_timesteps:], test_data_norm), axis=1),\n                                        in_timesteps, OUT_QUANT, used_feat, target_feat)\n            test_set = TimeSeriesDataset(pd.concatenate((train_data.iloc[-in_timesteps:], test_data), axis=1),\n                                        in_timesteps, OUT_QUANT, used_feat, target_feat)\n            \n            nnetwork = deepcopy(nnetwork_template).to(DEVICE)\n            optimizer = optimizer_creator(nnetwork.parameters())\n            scheduler = scheduler_creator(optimizer)\n\n            # Create an instance of the BackpropTraining class\n            backprop_training = BackpropTraining(nnetwork, scheduler, criterion, optimizer,\n                                                 train_set_norm, test_set_norm, test_set, global_verbosity=False)\n            # Train the model\n            backprop_training.train_validate_cycle(epochs, cycle_epoch=1, cycle_batch=9, verbosity=False)\n            # Check if the current model has a better average RMSE on the test set\n            str_end = '_' + \"_\".join(used_feat)\n            if backprop_training.best_rmse < best_average_rmse_test: # If the model is better save it\n                best_average_rmse_test = backprop_training.best_rmse\n                best_prev_steps_used = in_timesteps\n                backprop_training = BackpropTraining(nnetwork, scheduler, criterion, optimizer,\n                    train_set_norm, test_set_norm, test_set,\n                    checkpoint_dir=f\"{LAB_THREE_DIR}/{type(nnetwork).__name__}_checkpoints/epoch_best_cpt\",\n                    global_verbosity=False)\n                backprop_training.checkpoint(current_epoch=backprop_training.start_epoch,\n                        status=None,\n                        verbosity=False,\n                        directory_name=f\"{type(nnetwork).__name__}_GS_checkpoints\"+str_end) # Save the best epoch of current model as current best model\n\n            del nnetwork\n            del backprop_training\n            del optimizer\n            del scheduler\n            gc.collect()\n            torch.cuda.empty_cache()\n\n        return best_average_rmse_test, best_prev_steps_used\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LSTM/GRU network","metadata":{}},{"cell_type":"code","source":"class RNNetwork(nn.Module):\n    \"\"\"\n    Class that realizes RNN.\n    Default settings is LSTM with one layer and 3 input channels.\n    \"\"\"\n    def __init__(\n        self,\n        hidden_channels=50,\n        num_layers=1,\n        rnn_type=nn.LSTM,\n        input_channels=IN_QUANT,\n        output_dim=OUT_QUANT,\n        bidirectional=False\n    ):\n        super().__init__()\n        self.rnns = rnn_type(\n            input_size=input_channels, \n            hidden_size=hidden_channels, \n            bidirectional=bidirectional, \n            num_layers=num_layers,\n            batch_first=True\n        )\n        self.regressor = nn.Linear(hidden_channels * 2 if bidirectional else hidden_channels, output_dim)\n        \n    def forward(self, x):\n        self.rnns.flatten_parameters()\n        x = self.rnns(x)[0]    # first element is output of LSTM/GRU\n        x = x[:,-1,:]          # dimensions: [batch size, window size (we need last element), features number]\n        x = self.regressor(x)\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TimeDistributed(nn.Module):\n    def __init__(self, module, batch_first=True):\n        super(TimeDistributed, self).__init__()\n        self.module = module\n        self.batch_first = batch_first\n\n    def forward(self, x):\n\n        if len(x.size()) <= 2:\n            return self.module(x)\n\n        # Squash samples and timesteps into a single axis\n        x_reshape = x.contiguous().view(-1, x.size(-1))  # (samples * timesteps, input_size)\n\n        y = self.module(x_reshape)\n\n        # We have to reshape Y\n        if self.batch_first:\n            y = y.contiguous().view(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n        else:\n            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n\n        return y\n\nclass LSTMEncoderDecoder(nn.Module):\n    def __init__(self, input_size, hidden_size, out_seq_length, num_encoder_layers=1, num_decoder_layers=1, output_features_quant=1):\n        super(LSTMEncoderDecoder, self).__init__()\n        \n        self.hidden_size = hidden_size\n        self.out_seq_length = out_seq_length\n        self.output_features_quant = output_features_quant\n        \n        # LSTM encoder\n        self.encoder = nn.LSTM(input_size, hidden_size, num_encoder_layers)\n        # LSTM decoder\n        self.decoder = nn.LSTM(hidden_size, hidden_size, num_decoder_layers)\n        # Linear layer to map hidden state to output\n        self.linear = TimeDistributed(nn.Linear(hidden_size, output_features_quant))\n        \n    def forward(self, input_seq):\n        # Initialize hidden state and cell state for encoder\n        encoder_hidden = self.init_hidden()\n        # Pass input sequence through encoder\n        self.encoder.flatten_parameters()\n        _, encoder_hidden = self.encoder(input_seq, encoder_hidden)\n        # Initialize input for decoder\n        decoder_input = input_seq[-1].repeat(self.out_seq_length, 1).unsqueeze(0) # Use the last input step as the initial decoder input\n        # Feed through decoder\n        self.decoder.flatten_parameters()\n        decoder_output, _ = self.decoder(decoder_input, encoder_hidden)\n        return self.linear(decoder_output)\n    \n    def init_hidden(self):\n        # Initialize hidden state and cell state with zeros\n        hidden = torch.zeros(1, 1, self.hidden_size)\n        cell = torch.zeros(1, 1, self.hidden_size)\n        \n        return (hidden, cell)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"USED_TIMESTEPS_ARR = [7, 14, 21, 28]\nNETWORK_TEMPLATE = RNNetwork().to(DEVICE)\ndict_best_steps = {}\n\nfor univariate_variable in ALL_VARIABLE_NAMES:\n    best_average_rmse_test, best_prev_steps_used = BackpropTraining.find_best_model(\n            used_timesteps_arr=USED_TIMESTEPS_ARR,\n            train_data_norm=df_train_norm,\n            test_data_norm=df_test_norm,\n            test_data=df_test,\n            nnetwork_template=NETWORK_TEMPLATE,\n            epochs=50,\n            used_feat=list(univariate_variable),\n            target_feat=TARGET_NAME\n    )\n\n    dict_best_steps[univariate_variable] = best_prev_steps_used\n    \n    # Better run in single cells\n    train_set_norm = TimeSeriesDataset(train_data_norm, best_prev_steps_used, OUT_QUANT, univariate_variable, TARGET_NAME)\n    test_set_norm = TimeSeriesDataset(pd.concatenate((train_data_norm.iloc[-in_timesteps:], test_data_norm), axis=1),\n                                best_prev_steps_used, OUT_QUANT, univariate_variable, TARGET_NAME)\n    test_set = TimeSeriesDataset(pd.concatenate((train_data.iloc[-in_timesteps:], test_data), axis=1),\n                                best_prev_steps_used, OUT_QUANT, univariate_variable, TARGET_NAME)\n\n    nnetwork = deepcopy(NETWORK_TEMPLATE).to(DEVICE)\n    optimizer = GLOBAL_OPTIMIZER(nnetwork.parameters())\n    scheduler = GLOBAL_SCHEDULER(optimizer)    \n\n    backprop_training = BackpropTraining(nnetwork, scheduler, GLOBAL_CRITERION, optimizer,\n        train_set_norm, test_set_norm, test_set, \n        checkpoint_dir=f\"{LAB_THREE_DIR}/{type(nnetwork).__name__}_GS_checkpoints\" +\\\n                                         '_' + \"_\".join(used_feat) + \"/grid_search_best_cpt\",\n        global_verbosity=False)\n    \n    rmse_train, rmse_test, rmse_test_vec = backprop_training.plot_and_test()\n    print(f\"RMSE on the training set is: {rmse_train}\")\n    print(f\"RMSE on the testing set is: {rmse_test}\")\n    print(f\"RMSE for each of the 7 points on the testing set: {rmse_test_vec}\")","metadata":{},"execution_count":null,"outputs":[]}]}