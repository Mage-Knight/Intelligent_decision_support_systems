{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторна робота 7 з ІСППР\n",
    "### Виконали студенти групи КІ-31мп Шабо О.А. та Сотник Д.C.\n",
    "### Варіант 8 арифметичної нормалізації, HCR, мультиплікативний"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![6.png](Images\\\\6.png)\n",
    "![7.png](Images\\\\7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from copy import deepcopy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training using grid search and metrics calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e8b49cbb4e43ddbea59ffd3e4a617f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = datasets.load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "RAND_CONST = 10\n",
    "\n",
    "# Perform stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RAND_CONST)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'logistic_regression': LogisticRegression(max_iter=1000, random_state=RAND_CONST),\n",
    "    'svm': SVC(random_state=RAND_CONST),\n",
    "    'random_forest': RandomForestClassifier(random_state=RAND_CONST),\n",
    "    'knn': KNeighborsClassifier(),\n",
    "    'decision_tree': DecisionTreeClassifier(random_state=RAND_CONST)\n",
    "}\n",
    "\n",
    "# Define hyperparameters for each model\n",
    "params = {\n",
    "    'logistic_regression': {'classifier__C': [0.01, 0.1, 1, 10]},\n",
    "    'svm': {'classifier__C': [0.01, 0.1, 1, 10], 'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'classifier__gamma': ['scale', 'auto']},\n",
    "    'random_forest': {'classifier__n_estimators': [50, 100, 200, 350], 'classifier__max_depth': [None, 5, 10, 15, 20], 'classifier__min_samples_split': [2, 5, 10]},\n",
    "    'knn': {'classifier__n_neighbors': [3, 5, 7, 9, 11], 'classifier__weights': ['uniform', 'distance'], 'classifier__p': [1, 2]},\n",
    "    'decision_tree': {'classifier__max_depth': [None, 5, 10, 15, 20], 'classifier__min_samples_split': [2, 5, 10]}\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV and evaluate models\n",
    "metrics = {'f1': f1_score, 'roc_auc': roc_auc_score}\n",
    "results = {metric: {} for metric in metrics}\n",
    "\n",
    "for model in tqdm(models, total=len(models)):\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('selector', SelectKBest(f_classif, k=15)),\n",
    "        ('classifier', models[model])\n",
    "    ])\n",
    "    grid = GridSearchCV(pipe, params[model], cv=5)\n",
    "    grid.fit(X_train, y_train)\n",
    "    y_pred = grid.predict(X_test)\n",
    "    for metric in metrics:\n",
    "        results[metric][model] = metrics[metric](y_test, y_pred)  # Evaluate on the testing set\n",
    "\n",
    "# Normalize metric values to get weight vectors\n",
    "results_scaled = deepcopy(results)\n",
    "for metric in results:\n",
    "    values = np.array(list(results[metric].values()))\n",
    "    values_scaled = values / np.sum(values)\n",
    "    results_scaled[metric] = {model: weight for model, weight in zip(models, values_scaled)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric results f1_score and roc_auc_score for the five models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': {'logistic_regression': 0.965034965034965,\n",
       "  'svm': 0.9659863945578231,\n",
       "  'random_forest': 0.951048951048951,\n",
       "  'knn': 0.9444444444444444,\n",
       "  'decision_tree': 0.9115646258503401},\n",
       " 'roc_auc': {'logistic_regression': 0.9553571428571429,\n",
       "  'svm': 0.945436507936508,\n",
       "  'random_forest': 0.9365079365079365,\n",
       "  'knn': 0.9246031746031746,\n",
       "  'decision_tree': 0.8700396825396826}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': {'logistic_regression': 0.2036764029150177,\n",
       "  'svm': 0.20387720780796362,\n",
       "  'random_forest': 0.20072457098871313,\n",
       "  'knn': 0.19933065035684705,\n",
       "  'decision_tree': 0.19239116793145863},\n",
       " 'roc_auc': {'logistic_regression': 0.20625401584921824,\n",
       "  'svm': 0.20411222959948597,\n",
       "  'random_forest': 0.20218462197472692,\n",
       "  'knn': 0.19961447847504818,\n",
       "  'decision_tree': 0.18783465410152067}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1.png](Images\\\\1.png)\n",
    "![2.png](Images\\\\2.png)\n",
    "![3.png](Images\\\\3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HM(weights):\n",
    "    return len(weights) / np.sum(weights)\n",
    "def HCI(weights):\n",
    "    return (HM(weights) - len(weights)) * (len(weights) + 1) / (len(weights) * (len(weights) - 1))\n",
    "def HCR(weights):\n",
    "    return HCI(weights) / 1.11 # we divide by 1.11 since n=5\n",
    "def test_coherence(comparison_matrix):\n",
    "    column_sums = np.sum(comparison_matrix, axis=0)\n",
    "    weights = 1 / column_sums\n",
    "    normalized_weights = weights / np.sum(weights) # Normalize the weights so they sum to 1\n",
    "    indicator = HCR(weights)\n",
    "    print(\"HCRN is coherent\") if indicator <= 0.1 else print(\"HCRN isn't coherent\") # the threshold is 0.1 since n=5\n",
    "    return normalized_weights, indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HCRN is coherent\n"
     ]
    }
   ],
   "source": [
    "# Define a matrix of comparisons for the \"ability to tweak model to your needs\" of each model\n",
    "comparison_matrix = np.array([\n",
    "    [1  , 1/2, 1/4, 2  , 1/3],\n",
    "    [2  , 1  , 1/3, 3  , 4  ],\n",
    "    [4  , 3  , 1  , 5  , 3  ],\n",
    "    [1/2, 1/3, 1/5, 1  , 1/4],\n",
    "    [3  , 1/4, 1/3, 4  , 1  ]\n",
    "])\n",
    "weights_for_subjective_criterion, HCR_value = test_coherence(comparison_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10050752, 0.20760569, 0.49858059, 0.07035526, 0.12295094])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_for_subjective_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07476880943311598"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HCR_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see the HCR_value = 0.074 < 0.1, so HCRN is coherent\n",
    "## Global weights\n",
    "![4.png](Images\\\\4.png)\n",
    "![5.png](Images\\\\5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_weights = np.array([0.45, 0.35, 0.2]) # weights for the three criteria\n",
    "# first: f1\n",
    "# second: roc_auc\n",
    "# third: subjective criterion\n",
    "def find_global_weights(results_scaled, weights_for_subjective_criterion, criterion_weights):\n",
    "    local_weights_matrix = np.zeros((len(criterion_weights), len(weights_for_subjective_criterion)))\n",
    "    for ind, metric in enumerate(results_scaled): # first two rows correspond to numerical metrics\n",
    "        metric_values = np.array(list(results_scaled[metric].values()))\n",
    "        local_weights_matrix[ind, :] = metric_values\n",
    "    local_weights_matrix[-1, :] = weights_for_subjective_criterion # third row corresponds to the subjective criterion\n",
    "    global_weight = np.prod(np.power(local_weights_matrix.T, criterion_weights), axis=1)\n",
    "    return local_weights_matrix.T, global_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic_regression global weight: 0.17762491169157274\n",
      "svm global weight: 0.20470003366483328\n",
      "random_forest global weight: 0.24139549345627243\n",
      "knn global weight: 0.16193281557599146\n",
      "decision_tree global weight: 0.17444205045820554\n"
     ]
    }
   ],
   "source": [
    "local_weights_matrix_T, global_weights = find_global_weights(results_scaled, weights_for_subjective_criterion, criterion_weights)\n",
    "for ind, model in enumerate(models):\n",
    "    print(f\"{model} global weight: {global_weights[ind]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2036764 , 0.20625402, 0.10050752],\n",
       "       [0.20387721, 0.20411223, 0.20760569],\n",
       "       [0.20072457, 0.20218462, 0.49858059],\n",
       "       [0.19933065, 0.19961448, 0.07035526],\n",
       "       [0.19239117, 0.18783465, 0.12295094]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_weights_matrix_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn < decision_tree < logistic_regression < svm < random_forest\n"
     ]
    }
   ],
   "source": [
    "num_models = len(models)\n",
    "model_counts = {model: 0 for model in models.keys()}\n",
    "\n",
    "for i in range(num_models):\n",
    "    for j in range(i+1, num_models):\n",
    "        if np.prod(np.power(local_weights_matrix_T[i, :] / local_weights_matrix_T[j, :], criterion_weights)) >= 1:\n",
    "            model_counts[list(models.keys())[i]] += 1\n",
    "        else:\n",
    "            model_counts[list(models.keys())[j]] += 1\n",
    "\n",
    "# Sort models based on count\n",
    "sorted_models = sorted(model_counts.items(), key=lambda x: x[1])\n",
    "\n",
    "# Format output\n",
    "output = \" < \".join([model for model, count in sorted_models])\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigDataEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
